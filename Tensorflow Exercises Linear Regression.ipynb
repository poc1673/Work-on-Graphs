{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Exercises: Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scope: \n",
    "Use Tensorflow 2.0 and pandas to build a simple linear regression model and then test the results on a hold-out set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Optimization\n",
    "\n",
    "To \"warm-up\", I'll use the tensorflow framework to solve a simple optimization problem. This will be done with the analytic gradient, and the autodifferentiation procedure which is standard in Tensorflow.\n",
    "\n",
    "First, define the function we will be using which is the Beale function:\n",
    "\n",
    "$$ f(x,y) = (1.5-x+xy)^2 + (2.25-x+xy^2)^2 + (2.625-x+xy^3)^2$$ \n",
    "\n",
    "The gradient is:\n",
    "\n",
    "$$  \\nabla f(x,y) =  \\begin{bmatrix}\n",
    "2 x (y^6 + y^4 - 2 y^3 - y^2 - 2 y + 3) + 5.25 y^3 + 4.5 y^2 + 3 y - 12.75 \\\\\n",
    " 6 x (x (y^5 + 0.666667 y^3 - y^2 - 0.333333 y - 0.333333) + 2.625 y^2 + 1.5 y + 0.5)\n",
    "\\end{bmatrix}  $$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_beale(x):\n",
    "    return (tf.convert_to_tensor( (1.5-x[0]+x[0]*x[1])**2 + (2.25-x[0]+x[0]*x[1]**2)**2 + (2.625-x[0]+x[0]*x[1]**3)**2) )\n",
    "\n",
    "def grad_beal(x):\n",
    "    dx = 2*x[0]*(x[1]**6 + x[1]**4 - 2*x[1]**3 - x[1]**2-2*x[1] + 3) + 5.25*x[1]**3+4.5*x[1]**2+3*x[1]-12.75\n",
    "    dy = 6*x[0]*(x[0]*(x[1]**5 + 2.0/3.0 * x[1]**3-x[1]**2-1.0/3.0 * x[1]-1.0/3.0) + 2.625*x[1]**2+1.5*x[1]+.5)\n",
    "    return(tf.convert_to_tensor([dx,dy]))\n",
    "\n",
    "def AD_beale(x):\n",
    "    x = tf.Variable(x,dtype = tf.float32) \n",
    "    with tf.GradientTape(persistent = True) as dv:\n",
    "        temp_beale = f_beale(x)\n",
    "    dx = dv.gradient(temp_beale, x)\n",
    "    return(dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see below that the results end up being the same for the derivatives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=bool, numpy=array([ True,  True])>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_beal([1,1]) == AD_beale([1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, we will attemp to solve the problem using stochastic gradient descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(2,) dtype=float32, numpy=array([2.9961617 , 0.49904037], dtype=float32)>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt = tf.keras.optimizers.SGD(learning_rate = .01)\n",
    "#opt = tf.keras.optimizers.Adam(learning_rate = .01)\n",
    "x_0 = tf.Variable([1.0,1.0],dtype = tf.float32)\n",
    "func_for_opt = lambda: f_beale(x_0)\n",
    "\n",
    "for epoch in range(1500):\n",
    "    opt.minimize(func_for_opt, [x_0]).numpy()\n",
    "\n",
    "x_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([156. 900. 132.], shape=(3,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x_vals = tf.Variable([1.0,2.0,3.0],name = \"x_vals\")\n",
    "x_vals_two = tf.Variable([2.0,3.0,4.0],name = \"x_vals_two\")\n",
    "with tf.GradientTape() as tape:\n",
    "    y = x_vals[0]**3+x_vals[2]*x_vals[1]**2 + 2*x_vals[2]+100\n",
    "    z_one = x_vals[1]*(4*(y) + x_vals[0]**2)    \n",
    "    y_two = x_vals_two[0]**3+x_vals_two[2]*x_vals_two[1]**2 + 2*x_vals_two[2]+100\n",
    "    z_two = x_vals_two[1]*(4*(y_two) + x_vals_two[0]**2)\n",
    "    z = z_one + z_two\n",
    "    \n",
    "a, b = tape.gradient(z , [z,x_vals_two]) \n",
    "print(b)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constrained Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next solve a constrained optimization problem which is a constrained version of the Rosenbrock banana function:\n",
    "\n",
    "$$ \\min_{x,y} f(x,y)  =  (1-x)^2 + 100(y-x^2)$$\n",
    "\n",
    "Subject to:\n",
    "$$ (x-1)^3 - y + 1 <=0 $$\n",
    "\n",
    "and:\n",
    "\n",
    "$$ x+y-2<=0 $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_rosenbrock(vals):\n",
    "    return(tf.convert_to_tensor(  (1-vals[0])**2 + 100*(vals[1]-vals[0]**2)  ) )\n",
    "\n",
    "def const1_rosenbrock(vals):    \n",
    "    return(tf.convert_to_tensor( 1.0*((vals[0]-vals[1])**3 - vals[1] + 1)   ))\n",
    "\n",
    "def const2_rosenbrock(vals):    \n",
    "    return(tf.convert_to_tensor( 1.0*(vals[0]+vals[1]-2)   ))\n",
    "\n",
    "def rosenbrock_lagrange(vals):\n",
    "    return(  f_rosenbrock(vals)+const1_rosenbrock(vals)+const2_rosenbrock(vals)   )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=-299.0>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_rosenbrock([2.0,1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.SGD(learning_rate = 1e-7,momentum = .9)\n",
    "#opt = tf.keras.optimizers.Adam(learning_rate = 5e-6)\n",
    "x_0 = tf.Variable([.1,.1 ],dtype = tf.float32)\n",
    "func_for_opt = lambda: tf.abs(f_rosenbrock(x_0))\n",
    "\n",
    "\n",
    "#for epoch in range(1000):\n",
    "i=1\n",
    "while (func_for_opt().numpy()>1e-4)&(i < 60000):\n",
    "#    print(i)\n",
    "    if i%1000 == 0:\n",
    "        print(\"At iteration \" + str(i) +  \" current results are \" + str(x_0.numpy() )+ \" with function value of \"+str( func_for_opt().numpy())  + \".\")\n",
    "    opt.minimize(func_for_opt, [x_0]).numpy()\n",
    "    i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 5000 current results are [1.0583223 1.119438 ] with function value of 0.0006868541.\n",
      "At iteration 10000 current results are [1.0583223 1.1194379] with function value of 0.000674814.\n",
      "At iteration 15000 current results are [1.0583256 1.1194365] with function value of 0.00016772747.\n",
      "At iteration 20000 current results are [1.058328  1.1194353] with function value of 0.00078471005.\n",
      "At iteration 25000 current results are [1.0583247 1.1194369] with function value of 6.9633126e-05.\n",
      "At iteration 30000 current results are [1.0583297 1.1194346] with function value of 0.0012120008.\n",
      "At iteration 35000 current results are [1.0583258 1.1194364] with function value of 0.00020335615.\n",
      "At iteration 40000 current results are [1.0583266 1.119436 ] with function value of 0.0004169941.\n",
      "At iteration 45000 current results are [1.058322  1.1194382] with function value of 0.0007818341.\n",
      "At iteration 50000 current results are [1.0583224 1.1194379] with function value of 0.00065122545.\n",
      "At iteration 55000 current results are [1.0583235 1.1194376] with function value of 0.000390172.\n",
      "At iteration 60000 current results are [1.0583243 1.1194371] with function value of 0.00016471744.\n",
      "At iteration 65000 current results are [1.0583248 1.1194367] with function value of 3.400445e-05.\n",
      "At iteration 70000 current results are [1.0583264 1.1194361] with function value of 0.00035753846.\n",
      "At iteration 75000 current results are [1.0583259 1.1194364] with function value of 0.00022694468.\n",
      "At iteration 80000 current results are [1.0583221 1.119438 ] with function value of 0.0007342696.\n",
      "At iteration 85000 current results are [1.0583168 1.1194407] with function value of 0.002111122.\n",
      "At iteration 90000 current results are [1.0583225 1.1194379] with function value of 0.0006276369.\n",
      "At iteration 95000 current results are [1.0583278 1.1194353] with function value of 0.000737533.\n",
      "Final results at iteration 100000 where the final results are [1.0583318 1.1194335] with function value of 0.0017700046.\n"
     ]
    }
   ],
   "source": [
    "opt = tf.keras.optimizers.SGD(learning_rate = 1e-8,momentum = .9)\n",
    "#opt = tf.keras.optimizers.Adam(learning_rate = 1e-6)\n",
    "x_0 = tf.Variable([1.1,1.1  ],dtype = tf.float32)\n",
    "func_for_opt = lambda: tf.abs(rosenbrock_lagrange(x_0))\n",
    "\n",
    "\n",
    "#for epoch in range(1000):\n",
    "i=1\n",
    "while (func_for_opt().numpy()>1e-6)&(i < 100000):\n",
    "#    print(i)\n",
    "    if i%5000 == 0:\n",
    "        print(\"At iteration \" + str(i) +  \" current results are \" + str(x_0.numpy() )+ \" with function value of \"+str( func_for_opt().numpy())  + \".\")\n",
    "    opt.minimize(func_for_opt, [x_0]).numpy()\n",
    "    i+=1\n",
    "\n",
    "print(\"Final results at iteration \" + str(i) +  \" where the final results are \" + str(x_0.numpy() )+ \" with function value of \"+str( func_for_opt().numpy())  + \".\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data/Format Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results above show the basic steps of working through a mathematical optimization problem. At this point, we pivot to the use of the mathematical optimization framework to solve a statistical problem. \n",
    "\n",
    "Simple linear regression uses the mean squared error function to find a linear relationship between a set of features and an outcome. This is defined as:\n",
    "\n",
    "$$ MSE(\\beta) = \\frac{\\sum_{i=1}^N (y_i - \\bar{\\beta}X_i )^2 }{N}$$\n",
    "\n",
    "Here, we step through a simple example of linear regression using tensorflow and an implementation of the MSE function.\n",
    "\n",
    "The functions are defined in the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(y_val ,x_val,weights):\n",
    "    output = tf.Variable(tf.tensordot(X, weights, axes=1 ),name = \"output\"   )\n",
    "    mse_val =  tf.reduce_mean( tf.square( output - y_val ) )\n",
    "    return(mse_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this numerical example, we'll use the cars data-set and fit a few different variables to predict the gas mileage. \n",
    "\n",
    "First step: Load the data into the environment and form the design matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv( 'cars.csv' )\n",
    "continuous_features = data[ [ \"Identification.Year\",\"Engine Information.Engine Statistics.Horsepower\",\"Engine Information.Engine Statistics.Torque\"] ].values / 100 \n",
    "X = np.concatenate( [ continuous_features ] , axis=1 )\n",
    "X = np.append(np.ones((X.shape[0],1) ) , X, axis=1)\n",
    "Y = data[ [ 'Fuel Information.City mpg' ] ].values\n",
    "\n",
    "# Perform basic subset selection in the code:\n",
    "train_features , test_features ,train_labels, test_labels = train_test_split( X , Y , test_size=0.2 )\n",
    "# Training data.\n",
    "X = tf.Variable( train_features , dtype=tf.float32 )\n",
    "Y = tf.Variable( train_labels , dtype=tf.float32 )                                                         \n",
    "# Testing data\n",
    "test_X = tf.Variable( test_features , dtype=tf.float32 ) \n",
    "test_Y = tf.Variable( test_labels , dtype=tf.float32 ) \n",
    "num_features = X.shape[1]\n",
    "# Define the coefficientst that we'll be starting with:\n",
    "weights = tf.Variable(tf.random.normal((num_features,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, we begin with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'trainable_variables'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-298-15c9744b312b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'trainable_variables'"
     ]
    }
   ],
   "source": [
    "tf.trainable_variables() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No gradients provided for any variable: ['Variable:0'].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-297-123f121e3502>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMSE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mweight_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mfunc_for_opt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_mse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmse_opt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_for_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mweight_vals\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(self, loss, var_list, grad_loss, name)\u001b[0m\n\u001b[1;32m    375\u001b[0m         loss, var_list=var_list, grad_loss=grad_loss)\n\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_clip_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[0;34m(self, grads_and_vars, name, experimental_aggregate_gradients)\u001b[0m\n\u001b[1;32m    511\u001b[0m       \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mnone\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mvariables\u001b[0m \u001b[0mhave\u001b[0m \u001b[0mgradients\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m     \"\"\"\n\u001b[0;32m--> 513\u001b[0;31m     \u001b[0mgrads_and_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_filter_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    514\u001b[0m     \u001b[0mvar_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36m_filter_grads\u001b[0;34m(grads_and_vars)\u001b[0m\n\u001b[1;32m   1268\u001b[0m   \u001b[0mfiltered\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfiltered\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1270\u001b[0;31m     raise ValueError(\"No gradients provided for any variable: %s.\" %\n\u001b[0m\u001b[1;32m   1271\u001b[0m                      ([v.name for _, v in grads_and_vars],))\n\u001b[1;32m   1272\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mvars_with_empty_grads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No gradients provided for any variable: ['Variable:0']."
     ]
    }
   ],
   "source": [
    "mse_opt = tf.keras.optimizers.SGD(learning_rate = 1e-7,momentum = .9)\n",
    "#output = tf.Variable(   tf.tensordot( x_val , weights , axes=1 )   , dtype = tf.float32)\n",
    "#opt = tf.keras.optimizers.Adam(learning_rate = 5e-6)\n",
    "weight_vals = weights\n",
    "def temp_mse(weight_vals):\n",
    "    return(MSE(Y,X,weight_vals))\n",
    "func_for_opt = lambda: tf.abs(temp_mse(weight_vals))\n",
    "mse_opt.minimize(func_for_opt, [weight_vals]).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No gradients provided for any variable: ['Variable:0'].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-272-c1d1ceb7161d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfunc_for_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m&\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m60000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mmse_opt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_for_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mweight_vals\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m1000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"At iteration \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m  \u001b[0;34m\" current results are \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_vals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0;34m\" with function value of \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mfunc_for_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;34m+\u001b[0m \u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(self, loss, var_list, grad_loss, name)\u001b[0m\n\u001b[1;32m    375\u001b[0m         loss, var_list=var_list, grad_loss=grad_loss)\n\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_clip_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[0;34m(self, grads_and_vars, name, experimental_aggregate_gradients)\u001b[0m\n\u001b[1;32m    511\u001b[0m       \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mnone\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mvariables\u001b[0m \u001b[0mhave\u001b[0m \u001b[0mgradients\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m     \"\"\"\n\u001b[0;32m--> 513\u001b[0;31m     \u001b[0mgrads_and_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_filter_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    514\u001b[0m     \u001b[0mvar_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36m_filter_grads\u001b[0;34m(grads_and_vars)\u001b[0m\n\u001b[1;32m   1268\u001b[0m   \u001b[0mfiltered\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfiltered\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1270\u001b[0;31m     raise ValueError(\"No gradients provided for any variable: %s.\" %\n\u001b[0m\u001b[1;32m   1271\u001b[0m                      ([v.name for _, v in grads_and_vars],))\n\u001b[1;32m   1272\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mvars_with_empty_grads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No gradients provided for any variable: ['Variable:0']."
     ]
    }
   ],
   "source": [
    "mse_opt = tf.keras.optimizers.SGD(learning_rate = 1e-7,momentum = .9)\n",
    "output = tf.Variable(   tf.tensordot( x_val , weights , axes=1 )   , dtype = tf.float32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#opt = tf.keras.optimizers.Adam(learning_rate = 5e-6)\n",
    "weight_vals = weights\n",
    "\n",
    "\n",
    "func_for_opt = lambda: tf.abs(temp_mse(output))\n",
    "mse_opt.minimize(func_for_opt, [weight_vals]).numpy()\n",
    "#for epoch in range(1000):\n",
    "i=1\n",
    "while (func_for_opt().numpy()>1e-4)&(i < 60000):\n",
    "    print(i)\n",
    "    mse_opt.minimize(func_for_opt, [weight_vals]).numpy()\n",
    "    if i%1000 == 0:\n",
    "        print(\"At iteration \" + str(i) +  \" current results are \" + str(weight_vals.numpy() )+ \" with function value of \"+str( func_for_opt().numpy())  + \".\")\n",
    "    i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=33.8419>"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func_for_opt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-34c49c8c84e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "Y = tf.constant([2.0,2.0,2.0,2.0],dtype = 'float32')\n",
    "y_pred = tf.Variable([1.0,2.0,2.0,2.0],dtype = 'float32')    \n",
    "with tf.GradientTape() as tape:\n",
    "    mse = tf.reduce_mean( tf.square( y_pred - Y ) )\n",
    "grads = tape.gradient(mse, y_pred) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-6ad151ac3b57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx_one\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mx_two\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mend_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_one\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_two\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "x_one = tf.constant([[1., 2., 3.]])\n",
    "x_two = tf.constant([[2., 2., 3.]])\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    end_res = mean_squared_error(Y = x_one, y_pred = x_two)\n",
    "    \n",
    "hold = tape.gradient(end_res, x_one,x_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-68013fd38f1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#y_pred = tf.Variable([1.0,2.0,3.0,4.0],dtype = tf.float32)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#Y = tf.Variable( [1.0,2.0,3.0,4.0], dtype=tf.float32 )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "#y_pred = tf.Variable([1.0,2.0,3.0,4.0],dtype = tf.float32)\n",
    "#Y = tf.Variable( [1.0,2.0,3.0,4.0], dtype=tf.float32 ) \n",
    "output = tf.Variable( output, dtype=tf.float32 ) \n",
    "y_batch = tf.Variable( y_batch, dtype=tf.float32 ) \n",
    "with tf.GradientTape() as tape:\n",
    "    mse_val = tf.reduce_mean( tf.square( output - y_batch ) )\n",
    "    \n",
    "tape.gradient(mse_val,output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[-0.86452055]], dtype=float32)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error_deriv( y_batch , output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Linear Regression\n",
    "\n",
    "The example below shows how to perform a simple linear regression in tensorflow using the cars dataset. In this case, we will use the analytic derivative of the function but later on, we will utilize auto differentiation function to step down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = pd.read_csv( 'cars.csv' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features below are the variables we will use for the regression. This begins as an nd array but note that we add them to the craph as a constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_features = data[ [ \"Identification.Year\",\"Engine Information.Engine Statistics.Horsepower\",\"Engine Information.Engine Statistics.Torque\"] ].values / 100 \n",
    "X = np.concatenate( [ continuous_features ] , axis=1 )\n",
    "X = np.append(np.ones((X.shape[0],1) ) , X, axis=1)\n",
    "Y = data[ [ 'Fuel Information.City mpg' ] ].values\n",
    "train_features , test_features ,train_labels, test_labels = train_test_split( X , Y , test_size=0.2 )\n",
    "X = tf.constant( train_features , dtype=tf.float32 )\n",
    "Y = tf.constant( train_labels , dtype=tf.float32 )                                                         \n",
    "test_X = tf.constant( test_features , dtype=tf.float32 ) \n",
    "test_Y = tf.constant( test_labels , dtype=tf.float32 ) \n",
    "\n",
    "# Define the weights for the function:\n",
    "weights = tf.random.normal((num_features+1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4060, 3), dtype=float32, numpy=\n",
       "array([[20.1 ,  2.61,  2.81],\n",
       "       [20.12,  1.7 ,  1.77],\n",
       "       [20.1 ,  3.1 ,  3.65],\n",
       "       ...,\n",
       "       [20.12,  4.03,  4.17],\n",
       "       [20.12,  1.85,  1.9 ],\n",
       "       [20.11,  2.2 ,  2.58]], dtype=float32)>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the functions that are being used to optimize linear regression. In this case, we use MSE:\n",
    "\n",
    "$$ MSE(\\beta) = \\frac{\\sum_{i=1}^N (y_i - \\bar{\\beta}X_i )^2 }{N}$$\n",
    "\n",
    "\n",
    "The gradient of the MSE is composed of two different parts. First, the derivative with respect to the constant:\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial\\alpha }MSE(\\beta)= -(y_i - \\beta X_i) $$\n",
    "\n",
    "The partial derivative for each coefficient is similarly derived:\n",
    "$$ \\frac{\\partial}{\\partial\\beta_j }MSE(\\beta)= -2x_j (y_i - \\beta X_i) $$\n",
    "\n",
    "The functions are defined in the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error( Y , y_pred ):\n",
    "    return tf.reduce_mean( tf.square( y_pred - Y ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(y_val ,x_val, weights):\n",
    "    output = tf.Variable(   tf.tensordot( x_val , weights , axes=1 )   , dtype = tf.float32)\n",
    "    mse_val = tf.reshape( tf.reduce_mean( tf.square( output - y_val ) )  , [ 1 , 1 ] )  \n",
    "    return(mse_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[593.0495]], dtype=float32)>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE(y_val = Y, x_val = X,weights =  weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(1, 1) dtype=float32, numpy=array([[593.0494]], dtype=float32)>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def MSE(y_val ,x_val, weights):\n",
    "    output = tf.Variable(   np.tensordot( x_val , weights , axes=1 )   , dtype = np.float32)\n",
    "    mse_val = tf.Variable(np.reshape( np.mean( np.square( output - y_val ) )  , [ 1 , 1 ] )   , dtype = np.float32   )\n",
    "    return(mse_val)\n",
    "\n",
    "MSE(y_val = Y, x_val = X,weights =  weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    tape.watch(weights)\n",
    "    mse_result = MSE(y_val = Y, x_val = X,weights =  weights)\n",
    "\n",
    "print(tape.gradient(mse_result, weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(4060, 1) dtype=float32, numpy=\n",
       "array([[-7.4632063],\n",
       "       [-7.8094263],\n",
       "       [-6.0223713],\n",
       "       ...,\n",
       "       [-6.297119 ],\n",
       "       [-6.919585 ],\n",
       "       [-7.606163 ]], dtype=float32)>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.Variable(   np.tensordot( X , weights , axes=1 )   , dtype = np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(true, predicted):\n",
    "        return tf.reduce_mean(tf.square(true-predicted))      \n",
    "    \n",
    "with tf.GradientTape(persistent=True) as g:\n",
    "    loss = mse(Y,  tf.Variable(   np.tensordot( X , weights , axes=1 )   , dtype = np.float32) )\n",
    "    \n",
    "g.gradient(loss, weights)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "continuous_features = data[ [ \"Identification.Year\",\"Engine Information.Engine Statistics.Horsepower\",\"Engine Information.Engine Statistics.Torque\"] ].values / 100 \n",
    "type(continuous_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(4060, 1) dtype=float32, numpy=\n",
       "array([[-7.4632063],\n",
       "       [-7.8094263],\n",
       "       [-6.022372 ],\n",
       "       ...,\n",
       "       [-6.297119 ],\n",
       "       [-6.919585 ],\n",
       "       [-7.606163 ]], dtype=float32)>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.Variable(   tf.tensordot( X , weights , axes=1 )   , dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dMSE(y_val ,x_val, weights):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is 32.58639144897461\n",
      "Loss is 14.956770896911621\n",
      "Loss is 10.775121688842773\n",
      "Loss is 9.761115074157715\n",
      "Loss is 9.497570991516113\n",
      "Loss is 9.438419342041016\n",
      "Loss is 9.424309730529785\n",
      "Loss is 9.335890769958496\n",
      "Loss is 9.53090763092041\n",
      "Loss is 9.39068603515625\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Analytic gradient of the mean squared error:\n",
    "def mean_squared_error_deriv( Y , y_pred ):\n",
    "    return tf.reshape( tf.reduce_mean( 2 * ( y_pred - Y ) ) , [ 1 , 1 ] )    \n",
    "\n",
    "# Generate approximation of the derivative using backpropogration:\n",
    " \n",
    "# Apply matrix multiplication operation to the vector of betas. Add bias term instead of creating an additional row of the design matrix.\n",
    "def h ( X , weights , bias ):\n",
    "    return tf.tensordot( X , weights , axes=1 ) + bias\n",
    "\n",
    "# Arbitrary choices. Note to self: How to optimize these for performance/convergence?\n",
    "num_epochs = 10\n",
    "num_samples = X.shape[0]\n",
    "batch_size = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "# The data.Dataset call below makes the data available within Tensorflow and allows for transformations to be applied to it.\n",
    "dataset = tf.data.Dataset.from_tensor_slices(( X , Y )) \n",
    "dataset = dataset.shuffle( 500 ).repeat( num_epochs ).batch( batch_size )\n",
    "iterator = dataset.__iter__()\n",
    "\n",
    "num_features = X.shape[1]\n",
    "weights = tf.random.normal( ( num_features , 1 ) )\n",
    "bias = 0\n",
    "\n",
    "epochs_plot = list()\n",
    "loss_plot = list()\n",
    "\n",
    "for i in range( num_epochs ) :\n",
    "    \n",
    "    epoch_loss = list()\n",
    "    for b in range( int(num_samples/batch_size) ):\n",
    "        x_batch , y_batch = iterator.get_next()\n",
    "   \n",
    "        output = h( x_batch , weights , bias ) \n",
    "        loss = epoch_loss.append( mean_squared_error( y_batch , output ).numpy() )\n",
    "    \n",
    "    \n",
    "        output = tf.Variable( output, dtype=tf.float32 ) \n",
    "        y_batch = tf.Variable( y_batch, dtype=tf.float32 ) \n",
    "        with tf.GradientTape() as tape:\n",
    "            mse_val = tf.reshape( tf.reduce_mean( tf.square( output - y_batch ) )  , [ 1 , 1 ] )    \n",
    "    \n",
    "        dJ_dH = tape.gradient(mse_val,output)\n",
    "    \n",
    "  #      dJ_dH = mean_squared_error_deriv( y_batch , output)\n",
    "#        print(dJ_dH)\n",
    "        \n",
    "        \n",
    "        dH_dW = x_batch\n",
    "        dJ_dW = tf.reduce_mean( dJ_dH * dH_dW )\n",
    "        dJ_dB = tf.reduce_mean( dJ_dH )\n",
    "    \n",
    "        weights -= ( learning_rate * dJ_dW )\n",
    "        bias -= ( learning_rate * dJ_dB ) \n",
    "        \n",
    "    loss = np.array( epoch_loss ).mean()\n",
    "    epochs_plot.append( i + 1 )\n",
    "    loss_plot.append( loss ) \n",
    "    \n",
    "    print( 'Loss is {}'.format( loss ) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(5076, 1) dtype=float32, numpy=\n",
       "array([[-21.167173],\n",
       "       [-20.933979],\n",
       "       [-20.933979],\n",
       "       ...,\n",
       "       [-22.604477],\n",
       "       [-21.499138],\n",
       "       [-21.499138]], dtype=float32)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[77.11303]], dtype=float32)>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error_deriv(output_1,test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the calculation of the function goes in two main steps:\n",
    "1. Calculate the hat matrix using the h() function.\n",
    "2. Use the h() results (output) to produce y_pred.\n",
    "3. Use y_pred to calculate MSE.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two Stage Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the work, we will calculate a two stage regression. This process occurs using two different steps:\n",
    "\n",
    "1. Define an initial regression:\n",
    "$$ \\hat{y_1} = \\beta X$$\n",
    "2. Define a second set of regression variables using the residuals from step $1$, and a second set of explanatory variables, $z$:\n",
    "$$ \\hat{y_2} = (y - \\hat{y_1})*\\omega_1 + \\omega_{i\\neq1}z\\$$\n",
    "\n",
    "This can be convoluted since we have to fit regression #1, and then fit the second regerssion. In this instance, we will employ backpropagation to embed the residuals from the first regression into the formulation of the second problem.\n",
    "\n",
    "To do this, we set up the following optimization problem:\n",
    "\n",
    "$$ min(\\hat{y_2}  - y)^2$$\n",
    "\n",
    "Such that: \n",
    "$$ min(\\hat{y_1}  - y)^2 = 0$$\n",
    "\n",
    "\n",
    "Simplified version for comparison: Solve regression #1 first, and the #2 next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-3a5b2b317201>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mret_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0moutput_1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m  \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0moutput_2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'output_1' is not defined"
     ]
    }
   ],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    ret_val = tf.add(tf.reduce_mean( tf.square( output_1 - Y ) ) ,  tf.reduce_mean( tf.square( output_2 - Y ))) \n",
    "    \n",
    "a = tape.gradient(ret_val,output_1)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    mse_val = tf.reshape( tf.reduce_mean( tf.square( output - y_batch ) )  , [ 1 , 1 ] )   \n",
    "\n",
    "a = tape.gradient(mse_val,output)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test of the tape_gradient function. In the cast below, we are going to create a simple function with autodifferentiation which returns the function value, and the derivative of the final results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_X_two' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-b2266ac3d225>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_X_two\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'test_X_two' is not defined"
     ]
    }
   ],
   "source": [
    "type(test_X_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_X_one' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-9eba810b7b84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0moutput_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtest_X_one\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mweights_one\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mbias_1\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0moutput_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtest_X_two\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mweights_two\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mbias_2\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0moutput_1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_Y\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_X_one' is not defined"
     ]
    }
   ],
   "source": [
    "#tf.add(tf.reduce_mean( tf.square( y_pred_1 - Y ) ) ,  tf.reduce_mean( tf.square( y_pred_2 - Y ))\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    output_1 = h( test_X_one , weights_one , bias_1 )\n",
    "    output_2 = h( test_X_two , weights_two , bias_2 )\n",
    "    res = tf.reduce_mean( tf.square( output_1,test_Y ) )\n",
    "    res = tf.add(tf.reduce_mean( tf.square( output_1,test_Y ) ) ,  tf.reduce_mean( tf.square( output_2 - test_Y )))\n",
    "    \n",
    "w_one_grads  = tape.gradient(output_1 , weights_one)  \n",
    "print(w_one_grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_features = data[ [ \"Identification.Year\",\"Engine Information.Engine Statistics.Horsepower\"] ].values / 100 \n",
    "#categorical_research_features = data[ [ 'Research' ] ].values \n",
    "step1_vars = np.concatenate( [ continuous_features ] , axis=1 )\n",
    "step2_vars = np.concatenate( [ data[ [\"Engine Information.Engine Statistics.Torque\"] ] ], axis=1 )\n",
    "dep_var = data[ [ 'Fuel Information.City mpg' ] ].values\n",
    "test_X_one = tf.constant( step1_vars , dtype=tf.float32 ) \n",
    "test_X_two = tf.constant( step2_vars , dtype=tf.float32 ) \n",
    "test_Y = tf.constant( dep_var , dtype=tf.float32 )\n",
    "dataset = tf.data.Dataset.from_tensor_slices(( test_X_one , test_Y )) \n",
    "dataset = dataset.shuffle( 500 ).repeat( num_epochs ).batch( batch_size )\n",
    "iterator = dataset.__iter__()\n",
    "num_features_one = test_X_one.shape[1]\n",
    "num_features_two = test_X_two.shape[1]\n",
    "weights_one = tf.random.normal( ( num_features_one , 1 ) )\n",
    "weights_two = tf.random.normal( ( num_features_two , 1 ) )\n",
    "lambda_var = tf.random.normal( ( 1 , 1 ) )\n",
    "bias_1 = tf.random.normal( ( 1 , 1 ) )\n",
    "bias_2 = tf.random.normal( ( 1 , 1 ) )\n",
    "epochs_plot = list()\n",
    "loss_plot = list()\n",
    "output_1 = tf.Variable(h( test_X_one , weights_one , bias_1 ),tf.float32)\n",
    "output_2 = tf.Variable(h( test_X_one , weights_one , bias_2 ),tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-0.00669864]\n",
      " [-0.00704053]\n",
      " [-0.00704053]\n",
      " ...\n",
      " [-0.00464521]\n",
      " [-0.00626577]\n",
      " [-0.00626577]], shape=(5076, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    \n",
    "    output_1 =tf.Variable( h( test_X_one , weights_one , bias_1 ), dtype=tf.float32 )\n",
    "    output_2 = tf.Variable( h( test_X_two , weights_two , bias_2 ), dtype=tf.float32 )\n",
    "#    res = tf.reduce_mean( tf.square( output_1,test_Y ) )\n",
    "    res = tf.reshape( tf.add(tf.reduce_mean( tf.square( output_1,test_Y ) ) ,  tf.reduce_mean( tf.square( output_2 - test_Y ))) , [ 1 , 1 ] )   \n",
    "    \n",
    "print(tape.gradient(res , output_1)  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([156. 900. 132.], shape=(3,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x_vals = tf.Variable([1.0,2.0,3.0],name = \"x_vals\")\n",
    "x_vals_two = tf.Variable([2.0,3.0,4.0],name = \"x_vals_two\")\n",
    "with tf.GradientTape() as tape:\n",
    "    y = x_vals[0]**3+x_vals[2]*x_vals[1]**2 + 2*x_vals[2]+100\n",
    "    z_one = x_vals[1]*(4*(y) + x_vals[0]**2)    \n",
    "    y_two = x_vals_two[0]**3+x_vals_two[2]*x_vals_two[1]**2 + 2*x_vals_two[2]+100\n",
    "    z_two = x_vals_two[1]*(4*(y_two) + x_vals_two[0]**2)\n",
    "    z = z_one + z_two\n",
    "    \n",
    "a, b = tape.gradient(z , [x_vals,x_vals_two]) \n",
    "print(b)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1938.5131"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean( np.square( output_1 - test_Y ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=1938.5135>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_mean( tf.square( output_1 - test_Y ) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "w1 = tf.Variable(tf.random.normal( ( num_features_one , 1 ) ),name = \"w1\")\n",
    "w2 = tf.Variable(tf.random.normal( ( num_features_two, 1 ) ),name = \"w2\")\n",
    "bias_1 = tf.Variable(tf.random.normal( ( 1 , 1 ) ),name = \"bias_1\")\n",
    "bias_2 = tf.Variable(tf.random.normal( ( 1 , 1 ) ),name = \"bias_2\")\n",
    "output_1 =tf.Variable( h( test_X_one , w1 , bias_1 ), dtype=tf.float32 )\n",
    "output_2 = tf.Variable( h( test_X_two , w2 , bias_2 ), dtype=tf.float32 )\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "#    mse_val = tf.reshape( tf.reduce_mean( tf.square( output_1 - test_Y ) )  , [ 1 , 1 ] )\n",
    "#    mse_val = np.sum(output_1 - test_Y)**2 \n",
    "#    mse_val = w1[0]**2 + w1[1]*2+w1[0]*w1[1] + 10 + w2[0]**3\n",
    "    mse_val =  tf.Variable(np.mean( np.square( output_1 - test_Y ) ),dtype = tf.float32)\n",
    "results = tape.gradient(mse_val,w1)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "output = h( test_X_one , weights_one , bias_1 ) \n",
    "\n",
    "output_1 =tf.Variable( h( test_X_one , weights_one , bias_1 ), dtype=tf.float32 )\n",
    "output_2 = tf.Variable( h( test_X_two , weights_two , bias_2 ), dtype=tf.float32 )\n",
    "#loss = epoch_loss.append( mean_squared_error( test_Y , output_1 ).numpy() )\n",
    "#output = tf.Variable( output_1, dtype=tf.float32 ) \n",
    "y_batch = tf.Variable( test_Y, dtype=tf.float32 ) \n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(weights_one)\n",
    "    mse_val = tf.reshape( tf.reduce_mean( tf.square( output_1 - test_Y ) )  , [ 1 , 1 ] )    \n",
    "    \n",
    "dJ_dH = tape.gradient(mse_val,weights_one)\n",
    "print(dJ_dH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(5076, 1) dtype=float32, numpy=\n",
       "array([[17.685701],\n",
       "       [18.122816],\n",
       "       [18.122816],\n",
       "       ...,\n",
       "       [15.076742],\n",
       "       [17.14866 ],\n",
       "       [17.14866 ]], dtype=float32)>"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'Variable:0' shape=(2, 1) dtype=float32, numpy=\n",
       " array([[ 1.0401548],\n",
       "        [-0.8742276]], dtype=float32)>,\n",
       " <tf.Variable 'Variable:0' shape=(5076, 1) dtype=float32, numpy=\n",
       " array([[17.685701],\n",
       "        [18.122816],\n",
       "        [18.122816],\n",
       "        ...,\n",
       "        [15.076742],\n",
       "        [17.14866 ],\n",
       "        [17.14866 ]], dtype=float32)>)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tape.watched_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as tape:    \n",
    "    output_1 = tf.Variable(h( test_X_one , weights_one , bias_1 ),dtype = tf.float32)\n",
    "    output_2 = tf.Variable( h( test_X_two , weights_two , bias_2 ) ,dtype = tf.float32)\n",
    "    \n",
    "    #    final = tf.tensordot( test_X_one , weights_one , axes=1 ) + bias_1\n",
    "#    output_2 = h( test_X_two , weights_two , bias_2 )\n",
    "   # res = tf.reduce_mean( tf.square( output_1,test_Y ) )\n",
    "#    res = tf.add(tf.reduce_mean( tf.square( output_1,test_Y ) ) ,  tf.reduce_mean( tf.square( output_2 - test_Y )))\n",
    "    \n",
    "w_one_grads  = tape.gradient(output_1 , weights_one)  \n",
    "print(w_one_grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    ret_val = reg_lagrange( Y=y_batch , y_pred_1 = output_1, y_pred_2 = output_2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_plot = list()\n",
    "loss_plot = list()\n",
    "\n",
    "\n",
    "# Define lagrangian here by taking the sum of the two functions.\n",
    "def reg_lagrange( Y , y_pred_1, y_pred_2 ):\n",
    "    return tf.add(tf.reduce_mean( tf.square( y_pred_1 - Y ) ) ,  tf.reduce_mean( tf.square( y_pred_2 - Y ))) \n",
    "\n",
    "\n",
    "for i in range( num_epochs ) :    \n",
    "    epoch_loss = list()\n",
    "    for b in range( int(num_samples/batch_size) ):\n",
    "        x_batch , y_batch = iterator.get_next()\n",
    "#output = h( x_batch , weights , bias ) \n",
    "        output_1 = h( test_X_one , weights_one , bias_1 )\n",
    "        output_2 = h( test_X_one , weights_one , bias_2 )\n",
    "        loss = epoch_loss.append( reg_lagrange( Y=y_batch , y_pred_1 = output_1, y_pred_2 = output_2 ).numpy() )\n",
    "    \n",
    "        output = tf.Variable( output, dtype=tf.float32 ) \n",
    "        y_batch = tf.Variable( y_batch, dtype=tf.float32 ) \n",
    "        with tf.GradientTape() as tape:\n",
    "            mse_val = tf.reshape( tf.reduce_mean( tf.square( output - y_batch ) )  , [ 1 , 1 ] )   \n",
    "    output_1\n",
    "    \n",
    "        with tf.GradientTape() as tape:\n",
    "            ret_val = reg_lagrange( Y=y_batch , y_pred_1 = output_1, y_pred_2 = output_2 )\n",
    "    \n",
    "        dJ_dH = tape.gradient(mse_val,output)\n",
    "        #dJ_dH = mean_squared_error_deriv( y_batch , output)\n",
    "        dH_dW = x_batch\n",
    "        dJ_dW = tf.reduce_mean( dJ_dH * dH_dW )\n",
    "        dJ_dB = tf.reduce_mean( dJ_dH )\n",
    "    \n",
    "        weights -= ( learning_rate * dJ_dW )\n",
    "        bias -= ( learning_rate * dJ_dB ) \n",
    "    loss = np.array( epoch_loss ).mean()\n",
    "    epochs_plot.append( i + 1 )\n",
    "    loss_plot.append( loss ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_features = data[ [ \"Identification.Year\",\"Engine Information.Engine Statistics.Horsepower\"] ].values / 100 \n",
    "#categorical_research_features = data[ [ 'Research' ] ].values \n",
    "step1_vars = np.concatenate( [ continuous_features ] , axis=1 )\n",
    "step2_vars = np.concatenate( [ data[ [\"Engine Information.Engine Statistics.Torque\"] ] ], axis=1 )\n",
    "dep_var = data[ [ 'Fuel Information.City mpg' ] ].values\n",
    "test_X_one = tf.constant( step1_vars , dtype=tf.float32 ) \n",
    "\n",
    "num_epochs = 10\n",
    "num_samples = test_X_one.shape[0]\n",
    "batch_size = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "\n",
    "test_X_two = tf.constant( step2_vars , dtype=tf.float32 ) \n",
    "test_Y = tf.constant( dep_var , dtype=tf.float32 )\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(( test_X_one , test_Y )) \n",
    "dataset = dataset.shuffle( 500 ).repeat( num_epochs ).batch( batch_size )\n",
    "iterator = dataset.__iter__()\n",
    "num_features = test_X_one.shape[1]\n",
    "weights = tf.random.normal( ( num_features , 1 ) )\n",
    "bias = 0\n",
    "\n",
    "epochs_plot = list()\n",
    "loss_plot = list()\n",
    "\n",
    "for i in range( num_epochs ) :    \n",
    "    epoch_loss = list()\n",
    "    for b in range( int(num_samples/batch_size) ):\n",
    "        x_batch , y_batch = iterator.get_next()\n",
    "   \n",
    "        output = h( x_batch , weights , bias ) \n",
    "        loss = epoch_loss.append( mean_squared_error( y_batch , output ).numpy() )\n",
    "    \n",
    "        output = tf.Variable( output, dtype=tf.float32 ) \n",
    "        y_batch = tf.Variable( y_batch, dtype=tf.float32 ) \n",
    "        with tf.GradientTape() as tape:\n",
    "            mse_val = tf.reshape( tf.reduce_mean( tf.square( output - y_batch ) )  , [ 1 , 1 ] )   \n",
    "    \n",
    "    \n",
    "        dJ_dH = tape.gradient(mse_val,output)\n",
    "        #dJ_dH = mean_squared_error_deriv( y_batch , output)\n",
    "        dH_dW = x_batch\n",
    "        dJ_dW = tf.reduce_mean( dJ_dH * dH_dW )\n",
    "        dJ_dB = tf.reduce_mean( dJ_dH )\n",
    "    \n",
    "        weights -= ( learning_rate * dJ_dW )\n",
    "        bias -= ( learning_rate * dJ_dB ) \n",
    "    loss = np.array( epoch_loss ).mean()\n",
    "    epochs_plot.append( i + 1 )\n",
    "    loss_plot.append( loss ) \n",
    "    \n",
    "#    print( 'Loss is {}'.format( loss ) ) \n",
    "\n",
    "#Get predictions and then use them to create residuals.\n",
    "preds = bias +  weights[0]*step1_vars[:,0]+  weights[1]*step1_vars[:,1]     \n",
    "res_vals = preds-dep_var[:,0]        \n",
    "hold = pd.DataFrame(step2_vars)\n",
    "hold[\"res\"] = np.array(res_vals).tolist()\n",
    "test_X_two = tf.constant( np.concatenate( [ hold ], axis=1 ) , dtype=tf.float32 ) \n",
    " \n",
    "dataset_two = tf.data.Dataset.from_tensor_slices(( test_X_two , test_Y )) \n",
    "dataset_two = dataset_two.shuffle( 500 ).repeat( num_epochs ).batch( batch_size )\n",
    "iterator = dataset_two.__iter__()\n",
    "num_features = test_X_two.shape[1]\n",
    "weights = tf.random.normal( ( num_features , 1 ) )\n",
    "bias = 0\n",
    "        \n",
    "for i in range( num_epochs ) :    \n",
    "    epoch_loss = list()\n",
    "    for b in range( int(num_samples/batch_size) ):\n",
    "        x_batch , y_batch = iterator.get_next()\n",
    "   \n",
    "        output = h( x_batch , weights , bias ) \n",
    "        loss = epoch_loss.append( mean_squared_error( y_batch , output ).numpy() )\n",
    "    \n",
    "        output = tf.Variable( output, dtype=tf.float32 ) \n",
    "        y_batch = tf.Variable( y_batch, dtype=tf.float32 ) \n",
    "        with tf.GradientTape() as tape:\n",
    "            mse_val = tf.reshape( tf.reduce_mean( tf.square( output - y_batch ) )  , [ 1 , 1 ] )   \n",
    "    \n",
    "    \n",
    "        dJ_dH = tape.gradient(mse_val,output)\n",
    "        #dJ_dH = mean_squared_error_deriv( y_batch , output)\n",
    "        dH_dW = x_batch\n",
    "        dJ_dW = tf.reduce_mean( dJ_dH * dH_dW )\n",
    "        dJ_dB = tf.reduce_mean( dJ_dH )\n",
    "    \n",
    "        weights -= ( learning_rate * dJ_dW )\n",
    "        bias -= ( learning_rate * dJ_dB )      \n",
    "        \n",
    "        \n",
    "        \n",
    "    loss = np.array( epoch_loss ).mean()\n",
    "    epochs_plot.append( i + 1 )\n",
    "    loss_plot.append( loss ) \n",
    "    print( 'Loss is {}'.format( loss ) ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The version below solves the problem as a lagrange optimization problem:\n",
    "\n",
    "\n",
    "$$ min(\\hat{y_2}  - y)^2 + \\lambda(\\hat{y_1}  - \\beta X)^2$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def reg_lagrange_problem( Y , y_one_pred, y_two_pred ):\n",
    "    return tf.reduce_mean( tf.square( y_pred - Y ) ) + "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = bias +  weights[0]*step1_vars[:,0]+  weights[1]*step1_vars[:,1] \n",
    "res_vals = preds-dep_var[:,0]\n",
    "res_vals = np.array(res_vals)\n",
    "#res_vals.resize([5076,0])\n",
    "res_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(res_vals,step2_vars[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step2_vars[:,0]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.concatenate( [ data[ [\"Engine Information.Engine Statistics.Torque\"] ] ], axis=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hold = pd.DataFrame(step2_vars)\n",
    "hold[\"res\"] = res_vals.tolist()\n",
    "np.concatenate( [ hold ], axis=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.concatenate((step2_vars, res_vals), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(res_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(step2_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.hstack(res_vals,step2_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_reg(indeps,deps,num_epochs = 10,batch_size = 50, learning_rate = .001):\n",
    "    num_samples = indeps.shape[0]\n",
    "    indeps = tf.constant( indeps , dtype=tf.float32 ) \n",
    "    deps = tf.constant( deps , dtype=tf.float32 ) \n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices(( indeps , deps )) \n",
    "    dataset = dataset.shuffle( 500 ).repeat( num_epochs ).batch( batch_size )\n",
    "    iterator = dataset.__iter__()\n",
    "    num_features = indeps.shape[1]\n",
    "    weights = tf.random.normal( ( num_features , 1 ) )\n",
    "    bias = 0\n",
    "    epochs_plot = list()\n",
    "    loss_plot = list()\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_vals = bias +  weights[0]*step1_vars[:,0]+  weights[1]*step1_vars[:,1] - dep_var\n",
    "pred_vals - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
