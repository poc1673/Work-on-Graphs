{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Exercises: Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scope: \n",
    "Use Tensorflow 2.0 and pandas to build a simple linear regression model and then test the results on a hold-out set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data/Format Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, tensorflow as tf, numpy as np, sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dimensions.Height</th>\n",
       "      <th>Dimensions.Length</th>\n",
       "      <th>Dimensions.Width</th>\n",
       "      <th>Engine Information.Driveline</th>\n",
       "      <th>Engine Information.Engine Type</th>\n",
       "      <th>Engine Information.Hybrid</th>\n",
       "      <th>Engine Information.Number of Forward Gears</th>\n",
       "      <th>Engine Information.Transmission</th>\n",
       "      <th>Fuel Information.City mpg</th>\n",
       "      <th>Fuel Information.Fuel Type</th>\n",
       "      <th>Fuel Information.Highway mpg</th>\n",
       "      <th>Identification.Classification</th>\n",
       "      <th>Identification.ID</th>\n",
       "      <th>Identification.Make</th>\n",
       "      <th>Identification.Model Year</th>\n",
       "      <th>Identification.Year</th>\n",
       "      <th>Engine Information.Engine Statistics.Horsepower</th>\n",
       "      <th>Engine Information.Engine Statistics.Torque</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>140</td>\n",
       "      <td>143</td>\n",
       "      <td>202</td>\n",
       "      <td>All-wheel drive</td>\n",
       "      <td>Audi 3.2L 6 cylinder 250hp 236ft-lbs</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "      <td>6 Speed Automatic Select Shift</td>\n",
       "      <td>18</td>\n",
       "      <td>Gasoline</td>\n",
       "      <td>25</td>\n",
       "      <td>Automatic transmission</td>\n",
       "      <td>2009 Audi A3 3.2</td>\n",
       "      <td>Audi</td>\n",
       "      <td>2009 Audi A3</td>\n",
       "      <td>2009</td>\n",
       "      <td>250</td>\n",
       "      <td>236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>140</td>\n",
       "      <td>143</td>\n",
       "      <td>202</td>\n",
       "      <td>Front-wheel drive</td>\n",
       "      <td>Audi 2.0L 4 cylinder 200 hp 207 ft-lbs Turbo</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "      <td>6 Speed Automatic Select Shift</td>\n",
       "      <td>22</td>\n",
       "      <td>Gasoline</td>\n",
       "      <td>28</td>\n",
       "      <td>Automatic transmission</td>\n",
       "      <td>2009 Audi A3 2.0 T AT</td>\n",
       "      <td>Audi</td>\n",
       "      <td>2009 Audi A3</td>\n",
       "      <td>2009</td>\n",
       "      <td>200</td>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>140</td>\n",
       "      <td>143</td>\n",
       "      <td>202</td>\n",
       "      <td>Front-wheel drive</td>\n",
       "      <td>Audi 2.0L 4 cylinder 200 hp 207 ft-lbs Turbo</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "      <td>6 Speed Manual</td>\n",
       "      <td>21</td>\n",
       "      <td>Gasoline</td>\n",
       "      <td>30</td>\n",
       "      <td>Manual transmission</td>\n",
       "      <td>2009 Audi A3 2.0 T</td>\n",
       "      <td>Audi</td>\n",
       "      <td>2009 Audi A3</td>\n",
       "      <td>2009</td>\n",
       "      <td>200</td>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>140</td>\n",
       "      <td>143</td>\n",
       "      <td>202</td>\n",
       "      <td>All-wheel drive</td>\n",
       "      <td>Audi 2.0L 4 cylinder 200 hp 207 ft-lbs Turbo</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "      <td>6 Speed Automatic Select Shift</td>\n",
       "      <td>21</td>\n",
       "      <td>Gasoline</td>\n",
       "      <td>28</td>\n",
       "      <td>Automatic transmission</td>\n",
       "      <td>2009 Audi A3 2.0 T Quattro</td>\n",
       "      <td>Audi</td>\n",
       "      <td>2009 Audi A3</td>\n",
       "      <td>2009</td>\n",
       "      <td>200</td>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>140</td>\n",
       "      <td>143</td>\n",
       "      <td>202</td>\n",
       "      <td>All-wheel drive</td>\n",
       "      <td>Audi 2.0L 4 cylinder 200 hp 207 ft-lbs Turbo</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "      <td>6 Speed Automatic Select Shift</td>\n",
       "      <td>21</td>\n",
       "      <td>Gasoline</td>\n",
       "      <td>28</td>\n",
       "      <td>Automatic transmission</td>\n",
       "      <td>2009 Audi A3 2.0 T Quattro</td>\n",
       "      <td>Audi</td>\n",
       "      <td>2009 Audi A3</td>\n",
       "      <td>2009</td>\n",
       "      <td>200</td>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5071</th>\n",
       "      <td>13</td>\n",
       "      <td>253</td>\n",
       "      <td>201</td>\n",
       "      <td>Front-wheel drive</td>\n",
       "      <td>Honda 3.5L 6 Cylinder 250 hp 253 ft-lbs</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "      <td>5 Speed Automatic</td>\n",
       "      <td>18</td>\n",
       "      <td>Gasoline</td>\n",
       "      <td>25</td>\n",
       "      <td>Automatic transmission</td>\n",
       "      <td>2012 Honda Pilot EX-L</td>\n",
       "      <td>Honda</td>\n",
       "      <td>2012 Honda Pilot</td>\n",
       "      <td>2012</td>\n",
       "      <td>250</td>\n",
       "      <td>253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5072</th>\n",
       "      <td>141</td>\n",
       "      <td>249</td>\n",
       "      <td>108</td>\n",
       "      <td>All-wheel drive</td>\n",
       "      <td>Lamborghini 5.2L 10 cylinder 552 hp 398 ft-lbs</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "      <td>6 Speed Manual</td>\n",
       "      <td>12</td>\n",
       "      <td>Gasoline</td>\n",
       "      <td>20</td>\n",
       "      <td>Manual transmission</td>\n",
       "      <td>2012 Lamborghini Gallardo Coupe LP 560-4</td>\n",
       "      <td>Lamborghini</td>\n",
       "      <td>2012 Lamborghini Gallardo Coup</td>\n",
       "      <td>2012</td>\n",
       "      <td>552</td>\n",
       "      <td>398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5073</th>\n",
       "      <td>160</td>\n",
       "      <td>249</td>\n",
       "      <td>108</td>\n",
       "      <td>All-wheel drive</td>\n",
       "      <td>Lamborghini 5.2L 10 cylinder 552 hp 398 ft-lbs</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "      <td>6 Speed Manual</td>\n",
       "      <td>12</td>\n",
       "      <td>Gasoline</td>\n",
       "      <td>20</td>\n",
       "      <td>Manual transmission</td>\n",
       "      <td>2012 Lamborghini Gallardo LP 560-4 Spyder</td>\n",
       "      <td>Lamborghini</td>\n",
       "      <td>2012 Lamborghini Gallardo Spyder</td>\n",
       "      <td>2012</td>\n",
       "      <td>552</td>\n",
       "      <td>398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5074</th>\n",
       "      <td>200</td>\n",
       "      <td>210</td>\n",
       "      <td>110</td>\n",
       "      <td>Rear-wheel drive</td>\n",
       "      <td>BMW 3.0L 6 cylinder 315hp 330 ft-lbs Turbo</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "      <td>6 Speed Automatic Select Shift</td>\n",
       "      <td>17</td>\n",
       "      <td>Gasoline</td>\n",
       "      <td>25</td>\n",
       "      <td>Automatic transmission</td>\n",
       "      <td>2012 BMW 740i Sedan</td>\n",
       "      <td>BMW</td>\n",
       "      <td>2012 BMW 7 Series</td>\n",
       "      <td>2012</td>\n",
       "      <td>315</td>\n",
       "      <td>330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5075</th>\n",
       "      <td>200</td>\n",
       "      <td>94</td>\n",
       "      <td>110</td>\n",
       "      <td>Rear-wheel drive</td>\n",
       "      <td>BMW 3.0L 6 cylinder 315hp 330 ft-lbs Turbo</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "      <td>6 Speed Automatic Select Shift</td>\n",
       "      <td>17</td>\n",
       "      <td>Gasoline</td>\n",
       "      <td>25</td>\n",
       "      <td>Automatic transmission</td>\n",
       "      <td>2012 BMW 740Li Sedan</td>\n",
       "      <td>BMW</td>\n",
       "      <td>2012 BMW 7 Series</td>\n",
       "      <td>2012</td>\n",
       "      <td>315</td>\n",
       "      <td>330</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5076 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Dimensions.Height  Dimensions.Length  Dimensions.Width  \\\n",
       "0                   140                143               202   \n",
       "1                   140                143               202   \n",
       "2                   140                143               202   \n",
       "3                   140                143               202   \n",
       "4                   140                143               202   \n",
       "...                 ...                ...               ...   \n",
       "5071                 13                253               201   \n",
       "5072                141                249               108   \n",
       "5073                160                249               108   \n",
       "5074                200                210               110   \n",
       "5075                200                 94               110   \n",
       "\n",
       "     Engine Information.Driveline  \\\n",
       "0                 All-wheel drive   \n",
       "1               Front-wheel drive   \n",
       "2               Front-wheel drive   \n",
       "3                 All-wheel drive   \n",
       "4                 All-wheel drive   \n",
       "...                           ...   \n",
       "5071            Front-wheel drive   \n",
       "5072              All-wheel drive   \n",
       "5073              All-wheel drive   \n",
       "5074             Rear-wheel drive   \n",
       "5075             Rear-wheel drive   \n",
       "\n",
       "                      Engine Information.Engine Type  \\\n",
       "0               Audi 3.2L 6 cylinder 250hp 236ft-lbs   \n",
       "1       Audi 2.0L 4 cylinder 200 hp 207 ft-lbs Turbo   \n",
       "2       Audi 2.0L 4 cylinder 200 hp 207 ft-lbs Turbo   \n",
       "3       Audi 2.0L 4 cylinder 200 hp 207 ft-lbs Turbo   \n",
       "4       Audi 2.0L 4 cylinder 200 hp 207 ft-lbs Turbo   \n",
       "...                                              ...   \n",
       "5071         Honda 3.5L 6 Cylinder 250 hp 253 ft-lbs   \n",
       "5072  Lamborghini 5.2L 10 cylinder 552 hp 398 ft-lbs   \n",
       "5073  Lamborghini 5.2L 10 cylinder 552 hp 398 ft-lbs   \n",
       "5074      BMW 3.0L 6 cylinder 315hp 330 ft-lbs Turbo   \n",
       "5075      BMW 3.0L 6 cylinder 315hp 330 ft-lbs Turbo   \n",
       "\n",
       "      Engine Information.Hybrid  Engine Information.Number of Forward Gears  \\\n",
       "0                          True                                           6   \n",
       "1                          True                                           6   \n",
       "2                          True                                           6   \n",
       "3                          True                                           6   \n",
       "4                          True                                           6   \n",
       "...                         ...                                         ...   \n",
       "5071                       True                                           5   \n",
       "5072                       True                                           6   \n",
       "5073                       True                                           6   \n",
       "5074                       True                                           6   \n",
       "5075                       True                                           6   \n",
       "\n",
       "     Engine Information.Transmission  Fuel Information.City mpg  \\\n",
       "0     6 Speed Automatic Select Shift                         18   \n",
       "1     6 Speed Automatic Select Shift                         22   \n",
       "2                     6 Speed Manual                         21   \n",
       "3     6 Speed Automatic Select Shift                         21   \n",
       "4     6 Speed Automatic Select Shift                         21   \n",
       "...                              ...                        ...   \n",
       "5071               5 Speed Automatic                         18   \n",
       "5072                  6 Speed Manual                         12   \n",
       "5073                  6 Speed Manual                         12   \n",
       "5074  6 Speed Automatic Select Shift                         17   \n",
       "5075  6 Speed Automatic Select Shift                         17   \n",
       "\n",
       "     Fuel Information.Fuel Type  Fuel Information.Highway mpg  \\\n",
       "0                      Gasoline                            25   \n",
       "1                      Gasoline                            28   \n",
       "2                      Gasoline                            30   \n",
       "3                      Gasoline                            28   \n",
       "4                      Gasoline                            28   \n",
       "...                         ...                           ...   \n",
       "5071                   Gasoline                            25   \n",
       "5072                   Gasoline                            20   \n",
       "5073                   Gasoline                            20   \n",
       "5074                   Gasoline                            25   \n",
       "5075                   Gasoline                            25   \n",
       "\n",
       "     Identification.Classification                          Identification.ID  \\\n",
       "0           Automatic transmission                           2009 Audi A3 3.2   \n",
       "1           Automatic transmission                      2009 Audi A3 2.0 T AT   \n",
       "2              Manual transmission                         2009 Audi A3 2.0 T   \n",
       "3           Automatic transmission                 2009 Audi A3 2.0 T Quattro   \n",
       "4           Automatic transmission                 2009 Audi A3 2.0 T Quattro   \n",
       "...                            ...                                        ...   \n",
       "5071        Automatic transmission                      2012 Honda Pilot EX-L   \n",
       "5072           Manual transmission   2012 Lamborghini Gallardo Coupe LP 560-4   \n",
       "5073           Manual transmission  2012 Lamborghini Gallardo LP 560-4 Spyder   \n",
       "5074        Automatic transmission                        2012 BMW 740i Sedan   \n",
       "5075        Automatic transmission                       2012 BMW 740Li Sedan   \n",
       "\n",
       "     Identification.Make         Identification.Model Year  \\\n",
       "0                   Audi                      2009 Audi A3   \n",
       "1                   Audi                      2009 Audi A3   \n",
       "2                   Audi                      2009 Audi A3   \n",
       "3                   Audi                      2009 Audi A3   \n",
       "4                   Audi                      2009 Audi A3   \n",
       "...                  ...                               ...   \n",
       "5071               Honda                  2012 Honda Pilot   \n",
       "5072         Lamborghini    2012 Lamborghini Gallardo Coup   \n",
       "5073         Lamborghini  2012 Lamborghini Gallardo Spyder   \n",
       "5074                 BMW                 2012 BMW 7 Series   \n",
       "5075                 BMW                 2012 BMW 7 Series   \n",
       "\n",
       "      Identification.Year  Engine Information.Engine Statistics.Horsepower  \\\n",
       "0                    2009                                              250   \n",
       "1                    2009                                              200   \n",
       "2                    2009                                              200   \n",
       "3                    2009                                              200   \n",
       "4                    2009                                              200   \n",
       "...                   ...                                              ...   \n",
       "5071                 2012                                              250   \n",
       "5072                 2012                                              552   \n",
       "5073                 2012                                              552   \n",
       "5074                 2012                                              315   \n",
       "5075                 2012                                              315   \n",
       "\n",
       "      Engine Information.Engine Statistics.Torque  \n",
       "0                                             236  \n",
       "1                                             207  \n",
       "2                                             207  \n",
       "3                                             207  \n",
       "4                                             207  \n",
       "...                                           ...  \n",
       "5071                                          253  \n",
       "5072                                          398  \n",
       "5073                                          398  \n",
       "5074                                          330  \n",
       "5075                                          330  \n",
       "\n",
       "[5076 rows x 18 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"cars.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results shown above need to be turned into an array variable for Tensorflow so we access the values attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "indep_vars = df[[ \"Identification.Year\",\"Engine Information.Engine Statistics.Horsepower\",\"Engine Information.Engine Statistics.Torque\"]].values\n",
    "dep_var = df[\"Fuel Information.City mpg\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mean squared error function that is used to fit the model.\n",
    "def mean_squared_error( Y , y_pred ):\n",
    "    return tf.reduce_mean( tf.square( y_pred - Y ) )\n",
    "\n",
    "def mean_squared_error_deriv( Y , y_pred ):\n",
    "    return tf.reshape( tf.reduce_mean( 2 * ( y_pred - Y ) ) , [ 1 , 1 ] )\n",
    "    \n",
    "def h ( X , weights , bias ):\n",
    "    return tf.tensordot( X , weights , axes=1 ) + bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize hyperparameters for the model. We will be performing the following:\n",
    "\n",
    "1. Using Batch gradient descent with batches of size 10.\n",
    "2. Train the model with 100 repititions.\n",
    "3. We will use a step size of .001 for the gradient descent term.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "indep_vars = tf.constant(indep_vars, dtype = tf.float32)\n",
    "dep_vars = tf.constant(dep_var, dtype = tf.float32)\n",
    "\n",
    "num_epochs = 100\n",
    "num_samples = indep_vars.shape[0]\n",
    "batch_size = 10\n",
    "learning_rate = .001\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(( indep_vars , dep_var )) \n",
    "dataset = dataset.shuffle( 500 ).repeat( num_epochs ).batch( batch_size )\n",
    "iterator = dataset.__iter__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "cannot compute Sub as input #1(zero-based) was expected to be a float tensor but is a int64 tensor [Op:Sub]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-b3aac1b1554b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mx_batch\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mdJ_dH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_squared_error_deriv\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-7edae9025d1e>\u001b[0m in \u001b[0;36mmean_squared_error\u001b[0;34m(Y, y_pred)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Define the mean squared error function that is used to fit the model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmean_squared_error_deriv\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1125\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m         \u001b[0;31m# Even if dispatching the op failed, the RHS may be a tensor aware\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36msubtract\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_dispatch_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msubtract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36msub\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m  10454\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10455\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 10456\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m  10457\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10458\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6841\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6842\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6843\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6844\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: cannot compute Sub as input #1(zero-based) was expected to be a float tensor but is a int64 tensor [Op:Sub]"
     ]
    }
   ],
   "source": [
    "num_features = indep_vars.shape[1]\n",
    "weights = tf.random.normal((num_features,1))\n",
    "bias = 0\n",
    "epochs_plot = list()\n",
    "loss_plot = list()\n",
    "\n",
    "for i in range( num_epochs ) :\n",
    "    \n",
    "    epoch_loss = list()\n",
    "    for b in range( int(num_samples/batch_size) ):\n",
    "        x_batch , y_batch = iterator.get_next()\n",
    "   \n",
    "        output = h( x_batch , weights , bias ) \n",
    "        loss = epoch_loss.append( mean_squared_error( y_batch , output ).numpy() )\n",
    "    \n",
    "        dJ_dH = mean_squared_error_deriv( y_batch , output)\n",
    "        dH_dW = x_batch\n",
    "        dJ_dW = tf.reduce_mean( dJ_dH * dH_dW )\n",
    "        dJ_dB = tf.reduce_mean( dJ_dH )\n",
    "    \n",
    "        weights -= ( learning_rate * dJ_dW )\n",
    "        bias -= ( learning_rate * dJ_dB ) \n",
    "        \n",
    "    loss = np.array( epoch_loss ).mean()\n",
    "    epochs_plot.append( i + 1 )\n",
    "    loss_plot.append( loss ) \n",
    "    \n",
    "    print( 'Loss is {}'.format( loss ) ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review Regression Coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Regression Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of the use of gradient tape function:\n",
    "\n",
    "While, in general, we might seek to calculate the analytic derivative, it is often unfeasible or undesireable. In order to compensate, we can calculate the results using backpropogation. \n",
    "\n",
    "The chief function of this is GradientTape(). This function defines the computation of the gradient with respect to some inputs. \n",
    "\n",
    "A simple example: Get derivative of a quadratic equation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = tf.Variable(3.0)\n",
    "with tf.GradientTape() as tape:\n",
    "    w = z**2 + 3*z + 2\n",
    "    \n",
    "dy_dx = tape.gradient(w, z)\n",
    "dy_dx.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can actually add a third layer to this! Let's add something annoying and arbitrary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4572740300000.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = tf.Variable(3.0)\n",
    "with tf.GradientTape() as tape:\n",
    "    w = z**2 + 3*z + 2\n",
    "    second_level = 4**w+2**z+3*z*w+1\n",
    "    \n",
    "    \n",
    "dy_dx = tape.gradient(second_level,w, z)\n",
    "dy_dx.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can still get it to just stop at w, though:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = tf.Variable(3.0)\n",
    "with tf.GradientTape() as tape:\n",
    "    w = z**2 + 3*z + 2\n",
    "    second_level = 4**w+2**z+3*z*w+1\n",
    "    \n",
    "    \n",
    "dy_dx = tape.gradient(w, z)\n",
    "dy_dx.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can obviously be used on gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[-8.954019   -0.97417885]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "w = tf.Variable(tf.random.normal((3, 2)), name='w')\n",
    "b = tf.Variable(tf.zeros(2, dtype=tf.float32), name='b')\n",
    "x = [[1., 2., 3.]]\n",
    "print( x @ w + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's calculate the gradient of the MSE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = tf.constant([[2., 2., 3.]])\n",
    "y_pred = tf.Variable([1,2,3])\n",
    "\n",
    "\n",
    "def mean_squared_error_deriv( Y , y_pred ):\n",
    "    return tf.reshape( tf.reduce_mean( 2 * ( y_pred - Y ) ) , [ 1 , 1 ] )    \n",
    "\n",
    "def mean_squared_error_deriv( Y , y_pred ):\n",
    "    return tf.reshape( tf.reduce_mean( 2 * ( y_pred - Y ) ) , [ 1 , 1 ] )    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'list' and 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-ae366aac863b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmean_squared_error_deriv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-78-e1265044e528>\u001b[0m in \u001b[0;36mmean_squared_error_deriv\u001b[0;34m(Y, y_pred)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmean_squared_error_deriv\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'list' and 'list'"
     ]
    }
   ],
   "source": [
    "mean_squared_error_deriv(Y = [2., 2., 3.], y_pred = [1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0. 0. 0. 0.], shape=(4,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def mean_squared_error( Y , y_pred ):\n",
    "    return tf.reduce_mean( tf.square( y_pred - Y ) )\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([-0.5  0.   0.   0. ], shape=(4,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "Y = tf.constant([2.0,2.0,2.0,2.0],dtype = 'float32')\n",
    "y_pred = tf.Variable([1.0,2.0,2.0,2.0],dtype = 'float32')    \n",
    "with tf.GradientTape() as tape:\n",
    "    mse = tf.reduce_mean( tf.square( y_pred - Y ) )\n",
    "grads = tape.gradient(mse, y_pred) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_one = tf.constant([[1., 2., 3.]])\n",
    "x_two = tf.constant([[2., 2., 3.]])\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    end_res = mean_squared_error(Y = x_one, y_pred = x_two)\n",
    "    \n",
    "hold = tape.gradient(end_res, x_one,x_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(hold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4060, 1), dtype=float32, numpy=\n",
       "array([[12.],\n",
       "       [15.],\n",
       "       [26.],\n",
       "       ...,\n",
       "       [18.],\n",
       "       [12.],\n",
       "       [21.]], dtype=float32)>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(50, 1), dtype=float32, numpy=\n",
       "array([[16.263437 ],\n",
       "       [22.87446  ],\n",
       "       [14.933083 ],\n",
       "       [20.99522  ],\n",
       "       [15.14272  ],\n",
       "       [16.178055 ],\n",
       "       [12.628096 ],\n",
       "       [20.890894 ],\n",
       "       [25.122879 ],\n",
       "       [28.302065 ],\n",
       "       [16.058205 ],\n",
       "       [28.302065 ],\n",
       "       [12.562002 ],\n",
       "       [11.213196 ],\n",
       "       [19.673159 ],\n",
       "       [12.153124 ],\n",
       "       [12.203455 ],\n",
       "       [14.6565075],\n",
       "       [20.27971  ],\n",
       "       [17.736618 ],\n",
       "       [17.38979  ],\n",
       "       [24.154129 ],\n",
       "       [23.883762 ],\n",
       "       [15.776989 ],\n",
       "       [19.673159 ],\n",
       "       [17.310509 ],\n",
       "       [19.5929   ],\n",
       "       [19.088959 ],\n",
       "       [11.469494 ],\n",
       "       [17.738205 ],\n",
       "       [22.306158 ],\n",
       "       [16.26185  ],\n",
       "       [14.69145  ],\n",
       "       [24.155716 ],\n",
       "       [23.974167 ],\n",
       "       [23.560411 ],\n",
       "       [22.71113  ],\n",
       "       [12.094118 ],\n",
       "       [ 9.052976 ],\n",
       "       [ 9.455627 ],\n",
       "       [14.382736 ],\n",
       "       [14.933083 ],\n",
       "       [17.419239 ],\n",
       "       [19.089584 ],\n",
       "       [12.20187  ],\n",
       "       [12.560416 ],\n",
       "       [18.891312 ],\n",
       "       [12.34419  ],\n",
       "       [10.643914 ],\n",
       "       [12.092533 ]], dtype=float32)>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(50, 1), dtype=float32, numpy=\n",
       "array([[-0.06946251],\n",
       "       [ 0.2749784 ],\n",
       "       [-0.12267669],\n",
       "       [ 0.03980881],\n",
       "       [-0.19429119],\n",
       "       [-0.03287781],\n",
       "       [-0.5748762 ],\n",
       "       [ 0.35563576],\n",
       "       [ 0.40491515],\n",
       "       [ 0.65208256],\n",
       "       [ 0.16232818],\n",
       "       [ 0.53208256],\n",
       "       [-0.2975199 ],\n",
       "       [-0.47147214],\n",
       "       [ 0.26692635],\n",
       "       [-0.31387505],\n",
       "       [-0.3518618 ],\n",
       "       [-0.0537397 ],\n",
       "       [ 0.45118842],\n",
       "       [ 0.06946472],\n",
       "       [ 0.01559158],\n",
       "       [ 0.56616515],\n",
       "       [ 0.43535048],\n",
       "       [-0.00892044],\n",
       "       [ 0.42692634],\n",
       "       [ 0.01242035],\n",
       "       [ 0.26371595],\n",
       "       [ 0.00355835],\n",
       "       [-0.38122025],\n",
       "       [ 0.0695282 ],\n",
       "       [ 0.37224633],\n",
       "       [-0.06952599],\n",
       "       [-0.01234199],\n",
       "       [ 0.48622862],\n",
       "       [ 0.43896666],\n",
       "       [ 0.42241645],\n",
       "       [ 0.2684452 ],\n",
       "       [-0.35623527],\n",
       "       [-0.757881  ],\n",
       "       [-0.6217749 ],\n",
       "       [-0.10469055],\n",
       "       [-0.12267669],\n",
       "       [ 0.13676956],\n",
       "       [ 0.20358337],\n",
       "       [-0.3519252 ],\n",
       "       [-0.29758334],\n",
       "       [ 0.03565247],\n",
       "       [-0.4262324 ],\n",
       "       [-0.5342434 ],\n",
       "       [-0.35629866]], dtype=float32)>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#y_pred = tf.Variable([1.0,2.0,3.0,4.0],dtype = tf.float32)\n",
    "#Y = tf.Variable( [1.0,2.0,3.0,4.0], dtype=tf.float32 ) \n",
    "output = tf.Variable( output, dtype=tf.float32 ) \n",
    "y_batch = tf.Variable( y_batch, dtype=tf.float32 ) \n",
    "with tf.GradientTape() as tape:\n",
    "    mse_val = tf.reduce_mean( tf.square( output - y_batch ) )\n",
    "    \n",
    "tape.gradient(mse_val,output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.48277283]], dtype=float32)>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error_deriv( y_batch , output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is 81.42101287841797\n",
      "Loss is 72.92327117919922\n",
      "Loss is 69.03453063964844\n",
      "Loss is 63.12940216064453\n",
      "Loss is 58.98978805541992\n",
      "Loss is 55.490230560302734\n",
      "Loss is 51.039493560791016\n",
      "Loss is 47.757041931152344\n",
      "Loss is 44.857513427734375\n",
      "Loss is 41.416664123535156\n",
      "Loss is 39.01887130737305\n",
      "Loss is 36.765480041503906\n",
      "Loss is 34.76766586303711\n",
      "Loss is 32.530059814453125\n",
      "Loss is 30.165863037109375\n",
      "Loss is 29.10077667236328\n",
      "Loss is 27.17170524597168\n",
      "Loss is 26.260286331176758\n",
      "Loss is 24.8105411529541\n",
      "Loss is 23.622365951538086\n",
      "Loss is 23.105907440185547\n",
      "Loss is 21.522336959838867\n",
      "Loss is 20.898897171020508\n",
      "Loss is 20.23624038696289\n",
      "Loss is 19.429529190063477\n",
      "Loss is 18.677927017211914\n",
      "Loss is 17.74614715576172\n",
      "Loss is 17.66293716430664\n",
      "Loss is 17.229711532592773\n",
      "Loss is 16.668922424316406\n",
      "Loss is 16.346702575683594\n",
      "Loss is 15.848626136779785\n",
      "Loss is 15.572113990783691\n",
      "Loss is 15.224883079528809\n",
      "Loss is 14.993885040283203\n",
      "Loss is 14.937765121459961\n",
      "Loss is 14.286945343017578\n",
      "Loss is 14.15295124053955\n",
      "Loss is 14.07690715789795\n",
      "Loss is 13.762364387512207\n",
      "Loss is 13.941038131713867\n",
      "Loss is 13.71015453338623\n",
      "Loss is 13.295368194580078\n",
      "Loss is 13.18989086151123\n",
      "Loss is 13.188660621643066\n",
      "Loss is 13.150686264038086\n",
      "Loss is 12.864346504211426\n",
      "Loss is 12.932127952575684\n",
      "Loss is 13.065917015075684\n",
      "Loss is 12.70917797088623\n",
      "Loss is 12.62175178527832\n",
      "Loss is 12.775702476501465\n",
      "Loss is 12.438909530639648\n",
      "Loss is 12.581891059875488\n",
      "Loss is 12.485772132873535\n",
      "Loss is 12.356486320495605\n",
      "Loss is 12.35474681854248\n",
      "Loss is 12.229266166687012\n",
      "Loss is 12.284348487854004\n",
      "Loss is 12.268684387207031\n",
      "Loss is 12.337421417236328\n",
      "Loss is 12.301277160644531\n",
      "Loss is 12.355551719665527\n",
      "Loss is 12.39554500579834\n",
      "Loss is 12.18966007232666\n",
      "Loss is 12.202390670776367\n",
      "Loss is 12.091233253479004\n",
      "Loss is 12.249922752380371\n",
      "Loss is 12.294363975524902\n",
      "Loss is 12.160752296447754\n",
      "Loss is 12.205140113830566\n",
      "Loss is 12.021714210510254\n",
      "Loss is 12.109152793884277\n",
      "Loss is 12.043035507202148\n",
      "Loss is 12.093345642089844\n",
      "Loss is 12.103879928588867\n",
      "Loss is 12.027742385864258\n",
      "Loss is 12.06544017791748\n",
      "Loss is 12.252788543701172\n",
      "Loss is 12.034165382385254\n",
      "Loss is 12.15217399597168\n",
      "Loss is 11.911840438842773\n",
      "Loss is 12.156807899475098\n",
      "Loss is 11.822060585021973\n",
      "Loss is 12.21040153503418\n",
      "Loss is 12.059540748596191\n",
      "Loss is 12.145829200744629\n",
      "Loss is 12.057891845703125\n",
      "Loss is 12.178529739379883\n",
      "Loss is 12.087777137756348\n",
      "Loss is 12.031875610351562\n",
      "Loss is 12.160898208618164\n",
      "Loss is 12.244768142700195\n",
      "Loss is 12.042250633239746\n",
      "Loss is 12.20357894897461\n",
      "Loss is 12.239416122436523\n",
      "Loss is 12.186408996582031\n",
      "Loss is 12.086681365966797\n",
      "Loss is 12.15670394897461\n",
      "Loss is 12.021678924560547\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv( 'cars.csv' )\n",
    "data.head()\n",
    "\n",
    "continuous_features = data[ [ \"Identification.Year\",\"Engine Information.Engine Statistics.Horsepower\",\"Engine Information.Engine Statistics.Torque\"] ].values / 100 \n",
    "#categorical_research_features = data[ [ 'Research' ] ].values \n",
    "\n",
    "X = np.concatenate( [ continuous_features ] , axis=1 )\n",
    "Y = data[ [ 'Fuel Information.City mpg' ] ].values\n",
    "\n",
    "train_features , test_features ,train_labels, test_labels = train_test_split( X , Y , test_size=0.2 )\n",
    "\n",
    "X = tf.constant( train_features , dtype=tf.float32 )\n",
    "Y = tf.constant( train_labels , dtype=tf.float32 ) \n",
    "                                                          \n",
    "test_X = tf.constant( test_features , dtype=tf.float32 ) \n",
    "test_Y = tf.constant( test_labels , dtype=tf.float32 ) \n",
    "\n",
    "def mean_squared_error( Y , y_pred ):\n",
    "    return tf.reduce_mean( tf.square( y_pred - Y ) )\n",
    "# Analytic gradient of the mean squared error:\n",
    "def mean_squared_error_deriv( Y , y_pred ):\n",
    "    return tf.reshape( tf.reduce_mean( 2 * ( y_pred - Y ) ) , [ 1 , 1 ] )    \n",
    "\n",
    "# Generate approximation of the derivative using backpropogration:\n",
    " \n",
    "# Apply matrix multiplication operation to the vector of betas. Add bias term instead of creating an additional row of the design matrix.\n",
    "def h ( X , weights , bias ):\n",
    "    return tf.tensordot( X , weights , axes=1 ) + bias\n",
    "\n",
    "# Arbitrary choices. Note to self: How to optimize these for performance/convergence?\n",
    "num_epochs = 100\n",
    "num_samples = X.shape[0]\n",
    "batch_size = 1\n",
    "learning_rate = 0.001\n",
    "\n",
    "# The data.Dataset call below makes the data available within Tensorflow and allows for transformations to be applied to it.\n",
    "dataset = tf.data.Dataset.from_tensor_slices(( X , Y )) \n",
    "dataset = dataset.shuffle( 500 ).repeat( num_epochs ).batch( batch_size )\n",
    "iterator = dataset.__iter__()\n",
    "\n",
    "num_features = X.shape[1]\n",
    "weights = tf.random.normal( ( num_features , 1 ) )\n",
    "bias = 0\n",
    "\n",
    "epochs_plot = list()\n",
    "loss_plot = list()\n",
    "\n",
    "for i in range( num_epochs ) :\n",
    "    \n",
    "    epoch_loss = list()\n",
    "    for b in range( int(num_samples/batch_size) ):\n",
    "        x_batch , y_batch = iterator.get_next()\n",
    "   \n",
    "        output = h( x_batch , weights , bias ) \n",
    "        loss = epoch_loss.append( mean_squared_error( y_batch , output ).numpy() )\n",
    "    \n",
    "    \n",
    "        output = tf.Variable( output, dtype=tf.float32 ) \n",
    "        y_batch = tf.Variable( y_batch, dtype=tf.float32 ) \n",
    "        with tf.GradientTape() as tape:\n",
    "            mse_val = tf.reshape( tf.reduce_mean( tf.square( output - y_batch ) )  , [ 1 , 1 ] )    \n",
    "    \n",
    "        dJ_dH = tape.gradient(mse_val,output)\n",
    "    \n",
    "  #      dJ_dH = mean_squared_error_deriv( y_batch , output)\n",
    "#        print(dJ_dH)\n",
    "        \n",
    "        \n",
    "        dH_dW = x_batch\n",
    "        dJ_dW = tf.reduce_mean( dJ_dH * dH_dW )\n",
    "        dJ_dB = tf.reduce_mean( dJ_dH )\n",
    "    \n",
    "        weights -= ( learning_rate * dJ_dW )\n",
    "        bias -= ( learning_rate * dJ_dB ) \n",
    "        \n",
    "    loss = np.array( epoch_loss ).mean()\n",
    "    epochs_plot.append( i + 1 )\n",
    "    loss_plot.append( loss ) \n",
    "    \n",
    "    print( 'Loss is {}'.format( loss ) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is 39.903804779052734\n",
      "Loss is 39.946510314941406\n",
      "Loss is 39.99775314331055\n",
      "Loss is 39.776546478271484\n",
      "Loss is 40.196739196777344\n",
      "Loss is 39.92945098876953\n",
      "Loss is 39.87966537475586\n",
      "Loss is 40.149742126464844\n",
      "Loss is 39.769683837890625\n",
      "Loss is 39.77928161621094\n",
      "Loss is 40.140892028808594\n",
      "Loss is 39.89653396606445\n",
      "Loss is 40.01445388793945\n",
      "Loss is 39.97110366821289\n",
      "Loss is 40.02952575683594\n",
      "Loss is 39.89756774902344\n",
      "Loss is 39.82274627685547\n",
      "Loss is 40.13584899902344\n",
      "Loss is 39.74961853027344\n",
      "Loss is 39.86760711669922\n",
      "Loss is 40.324928283691406\n",
      "Loss is 40.04932403564453\n",
      "Loss is 39.83456039428711\n",
      "Loss is 39.68816375732422\n",
      "Loss is 40.21446228027344\n",
      "Loss is 39.97736740112305\n",
      "Loss is 39.53959655761719\n",
      "Loss is 40.21420669555664\n",
      "Loss is 39.90022277832031\n",
      "Loss is 39.67402267456055\n",
      "Loss is 40.03942108154297\n",
      "Loss is 40.137638092041016\n",
      "Loss is 40.00418472290039\n",
      "Loss is 39.95606231689453\n",
      "Loss is 39.80842208862305\n",
      "Loss is 40.097686767578125\n",
      "Loss is 39.55204391479492\n",
      "Loss is 40.20629119873047\n",
      "Loss is 39.891048431396484\n",
      "Loss is 39.99503707885742\n",
      "Loss is 39.24534225463867\n",
      "Loss is 40.199928283691406\n",
      "Loss is 40.40781784057617\n",
      "Loss is 39.632301330566406\n",
      "Loss is 39.851070404052734\n",
      "Loss is 40.20769119262695\n",
      "Loss is 39.720340728759766\n",
      "Loss is 40.09226608276367\n",
      "Loss is 39.69755935668945\n",
      "Loss is 40.139183044433594\n",
      "Loss is 40.04471969604492\n",
      "Loss is 39.84036636352539\n",
      "Loss is 39.7481803894043\n",
      "Loss is 39.888980865478516\n",
      "Loss is 40.09428787231445\n",
      "Loss is 39.730167388916016\n",
      "Loss is 40.027854919433594\n",
      "Loss is 39.767311096191406\n",
      "Loss is 39.497867584228516\n",
      "Loss is 40.31825256347656\n",
      "Loss is 39.980262756347656\n",
      "Loss is 39.401859283447266\n",
      "Loss is 40.34100341796875\n",
      "Loss is 39.47206115722656\n",
      "Loss is 40.02513885498047\n",
      "Loss is 39.915794372558594\n",
      "Loss is 39.863433837890625\n",
      "Loss is 39.64997863769531\n",
      "Loss is 39.78006362915039\n",
      "Loss is 40.209651947021484\n",
      "Loss is 39.74208450317383\n",
      "Loss is 39.5606689453125\n",
      "Loss is 40.36951446533203\n",
      "Loss is 39.2568473815918\n",
      "Loss is 40.32145690917969\n",
      "Loss is 39.94087600708008\n",
      "Loss is 40.14022445678711\n",
      "Loss is 39.950313568115234\n",
      "Loss is 39.655845642089844\n",
      "Loss is 40.07099533081055\n",
      "Loss is 39.747039794921875\n",
      "Loss is 39.831363677978516\n",
      "Loss is 40.193626403808594\n",
      "Loss is 39.86986541748047\n",
      "Loss is 39.76294708251953\n",
      "Loss is 39.79697799682617\n",
      "Loss is 39.84884262084961\n",
      "Loss is 40.594024658203125\n",
      "Loss is 39.15934753417969\n",
      "Loss is 40.14226150512695\n",
      "Loss is 39.727054595947266\n",
      "Loss is 40.09774398803711\n",
      "Loss is 40.19368362426758\n",
      "Loss is 39.88087463378906\n",
      "Loss is 39.632266998291016\n",
      "Loss is 39.95711135864258\n",
      "Loss is 40.139739990234375\n",
      "Loss is 40.010555267333984\n",
      "Loss is 39.5845947265625\n",
      "Loss is 39.81550979614258\n",
      "Loss is 39.936195373535156\n",
      "Loss is 39.709739685058594\n",
      "Loss is 39.87337112426758\n",
      "Loss is 40.015045166015625\n",
      "Loss is 39.89280700683594\n",
      "Loss is 39.75481414794922\n",
      "Loss is 39.85403823852539\n",
      "Loss is 39.92483901977539\n",
      "Loss is 39.48638153076172\n",
      "Loss is 40.209617614746094\n",
      "Loss is 39.774436950683594\n",
      "Loss is 39.70695114135742\n",
      "Loss is 40.19525146484375\n",
      "Loss is 40.07038497924805\n",
      "Loss is 39.19896697998047\n",
      "Loss is 40.10732650756836\n",
      "Loss is 39.747379302978516\n",
      "Loss is 39.737247467041016\n",
      "Loss is 40.235252380371094\n",
      "Loss is 39.46411895751953\n",
      "Loss is 40.429012298583984\n",
      "Loss is 39.47233200073242\n",
      "Loss is 39.94149398803711\n",
      "Loss is 39.649444580078125\n",
      "Loss is 39.67329025268555\n",
      "Loss is 39.813907623291016\n",
      "Loss is 40.02341079711914\n",
      "Loss is 40.3094482421875\n",
      "Loss is 39.395992279052734\n",
      "Loss is 40.20930099487305\n",
      "Loss is 39.74443435668945\n",
      "Loss is 39.32754898071289\n",
      "Loss is 40.194210052490234\n",
      "Loss is 39.9692497253418\n",
      "Loss is 39.599456787109375\n",
      "Loss is 39.81904983520508\n",
      "Loss is 39.45445251464844\n",
      "Loss is 40.275146484375\n",
      "Loss is 40.12907791137695\n",
      "Loss is 39.419921875\n",
      "Loss is 40.055397033691406\n",
      "Loss is 39.832862854003906\n",
      "Loss is 39.918983459472656\n",
      "Loss is 39.54418182373047\n",
      "Loss is 40.09319305419922\n",
      "Loss is 39.33549499511719\n",
      "Loss is 40.185977935791016\n",
      "Loss is 39.811519622802734\n",
      "Loss is 39.987701416015625\n",
      "Loss is 39.79731750488281\n",
      "Loss is 39.33441925048828\n",
      "Loss is 40.01427459716797\n",
      "Loss is 39.40091323852539\n",
      "Loss is 39.95145034790039\n",
      "Loss is 40.321537017822266\n",
      "Loss is 39.396324157714844\n",
      "Loss is 40.234153747558594\n",
      "Loss is 39.159114837646484\n",
      "Loss is 40.25859451293945\n",
      "Loss is 39.25617218017578\n",
      "Loss is 40.26963806152344\n",
      "Loss is 39.82816696166992\n",
      "Loss is 39.55754470825195\n",
      "Loss is 39.83549118041992\n",
      "Loss is 39.65237808227539\n",
      "Loss is 40.11092758178711\n",
      "Loss is 39.94956970214844\n",
      "Loss is 40.32001876831055\n",
      "Loss is 39.04338455200195\n",
      "Loss is 39.84288024902344\n",
      "Loss is 40.009246826171875\n",
      "Loss is 39.8690185546875\n",
      "Loss is 39.651451110839844\n",
      "Loss is 39.61659622192383\n",
      "Loss is 39.90411376953125\n",
      "Loss is 39.73440933227539\n",
      "Loss is 39.653743743896484\n",
      "Loss is 39.899810791015625\n",
      "Loss is 39.639808654785156\n",
      "Loss is 39.599151611328125\n",
      "Loss is 40.45496368408203\n",
      "Loss is 39.564918518066406\n",
      "Loss is 39.83570098876953\n",
      "Loss is 39.559417724609375\n",
      "Loss is 39.512908935546875\n",
      "Loss is 40.01217269897461\n",
      "Loss is 39.55929183959961\n",
      "Loss is 39.586116790771484\n",
      "Loss is 40.043601989746094\n",
      "Loss is 39.65530014038086\n",
      "Loss is 39.567108154296875\n",
      "Loss is 40.001495361328125\n",
      "Loss is 39.88511657714844\n",
      "Loss is 39.503971099853516\n",
      "Loss is 40.02959442138672\n",
      "Loss is 39.78001403808594\n",
      "Loss is 39.37876892089844\n",
      "Loss is 40.2026481628418\n",
      "Loss is 39.652061462402344\n",
      "Loss is 39.921958923339844\n",
      "Loss is 39.79872131347656\n",
      "Loss is 39.76688766479492\n",
      "Loss is 39.80078125\n",
      "Loss is 39.73939895629883\n",
      "Loss is 39.22386932373047\n",
      "Loss is 40.360443115234375\n",
      "Loss is 39.68404769897461\n",
      "Loss is 39.4342155456543\n",
      "Loss is 39.0973014831543\n",
      "Loss is 40.319480895996094\n",
      "Loss is 39.73188400268555\n",
      "Loss is 39.44371032714844\n",
      "Loss is 39.86884689331055\n",
      "Loss is 39.59089279174805\n",
      "Loss is 39.65254211425781\n",
      "Loss is 39.93568801879883\n",
      "Loss is 39.83586883544922\n",
      "Loss is 39.79419708251953\n",
      "Loss is 39.42933654785156\n",
      "Loss is 39.851016998291016\n",
      "Loss is 40.554683685302734\n",
      "Loss is 38.824100494384766\n",
      "Loss is 40.08325958251953\n",
      "Loss is 39.36051559448242\n",
      "Loss is 40.32649230957031\n",
      "Loss is 39.20595932006836\n",
      "Loss is 39.69287872314453\n",
      "Loss is 39.46316909790039\n",
      "Loss is 39.94791030883789\n",
      "Loss is 39.59502029418945\n",
      "Loss is 40.05880355834961\n",
      "Loss is 39.608089447021484\n",
      "Loss is 39.42888641357422\n",
      "Loss is 40.129825592041016\n",
      "Loss is 39.55507278442383\n",
      "Loss is 39.54794692993164\n",
      "Loss is 40.11549758911133\n",
      "Loss is 39.39628601074219\n",
      "Loss is 39.50687026977539\n",
      "Loss is 40.135616302490234\n",
      "Loss is 39.83925247192383\n",
      "Loss is 39.241058349609375\n",
      "Loss is 39.88033676147461\n",
      "Loss is 39.948081970214844\n",
      "Loss is 39.50225830078125\n",
      "Loss is 39.758567810058594\n",
      "Loss is 39.84967041015625\n",
      "Loss is 39.37611770629883\n",
      "Loss is 39.57277297973633\n",
      "Loss is 39.822784423828125\n",
      "Loss is 40.0902214050293\n",
      "Loss is 40.03548049926758\n",
      "Loss is 39.215003967285156\n",
      "Loss is 39.4134407043457\n",
      "Loss is 40.23188018798828\n",
      "Loss is 39.34627914428711\n",
      "Loss is 39.667991638183594\n",
      "Loss is 39.78466796875\n",
      "Loss is 39.759254455566406\n",
      "Loss is 39.62923812866211\n",
      "Loss is 39.624271392822266\n",
      "Loss is 39.47499084472656\n",
      "Loss is 39.46926498413086\n",
      "Loss is 39.74608612060547\n",
      "Loss is 40.001285552978516\n",
      "Loss is 39.52547073364258\n",
      "Loss is 39.6948356628418\n",
      "Loss is 39.703895568847656\n",
      "Loss is 40.069602966308594\n",
      "Loss is 39.511234283447266\n",
      "Loss is 39.68258285522461\n",
      "Loss is 40.00777816772461\n",
      "Loss is 39.62775421142578\n",
      "Loss is 39.443450927734375\n",
      "Loss is 39.89223098754883\n",
      "Loss is 39.22767639160156\n",
      "Loss is 39.77781295776367\n",
      "Loss is 39.387786865234375\n",
      "Loss is 40.04756164550781\n",
      "Loss is 39.55107879638672\n",
      "Loss is 39.5630989074707\n",
      "Loss is 39.73774719238281\n",
      "Loss is 39.46781539916992\n",
      "Loss is 39.842288970947266\n",
      "Loss is 39.81572341918945\n",
      "Loss is 39.27859878540039\n",
      "Loss is 39.791175842285156\n",
      "Loss is 39.95133590698242\n",
      "Loss is 39.95595932006836\n",
      "Loss is 39.30039978027344\n",
      "Loss is 39.567420959472656\n",
      "Loss is 39.93689727783203\n",
      "Loss is 39.52128982543945\n",
      "Loss is 39.80533981323242\n",
      "Loss is 39.4261589050293\n",
      "Loss is 39.582855224609375\n",
      "Loss is 39.465484619140625\n",
      "Loss is 39.9244384765625\n",
      "Loss is 39.4593620300293\n",
      "Loss is 40.0280647277832\n",
      "Loss is 39.60020446777344\n",
      "Loss is 39.50522994995117\n",
      "Loss is 39.209651947021484\n",
      "Loss is 39.94955062866211\n",
      "Loss is 39.66636657714844\n",
      "Loss is 39.223777770996094\n",
      "Loss is 39.97835159301758\n",
      "Loss is 39.96494674682617\n",
      "Loss is 39.43699645996094\n",
      "Loss is 39.766639709472656\n",
      "Loss is 39.33959197998047\n",
      "Loss is 39.86100387573242\n",
      "Loss is 39.413475036621094\n",
      "Loss is 39.61890411376953\n",
      "Loss is 39.78470230102539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is 39.93193435668945\n",
      "Loss is 39.53889846801758\n",
      "Loss is 39.09602737426758\n",
      "Loss is 39.99235153198242\n",
      "Loss is 39.48787307739258\n",
      "Loss is 39.66982650756836\n",
      "Loss is 40.08349609375\n",
      "Loss is 38.80995178222656\n",
      "Loss is 40.04288101196289\n",
      "Loss is 39.75446319580078\n",
      "Loss is 40.139095306396484\n",
      "Loss is 39.021793365478516\n",
      "Loss is 39.8248176574707\n",
      "Loss is 39.70288848876953\n",
      "Loss is 39.34148406982422\n",
      "Loss is 39.530250549316406\n",
      "Loss is 39.7408447265625\n",
      "Loss is 39.78431701660156\n",
      "Loss is 39.443695068359375\n",
      "Loss is 39.72011947631836\n",
      "Loss is 39.36939239501953\n",
      "Loss is 39.64442825317383\n",
      "Loss is 39.44346618652344\n",
      "Loss is 39.77347183227539\n",
      "Loss is 39.38665771484375\n",
      "Loss is 39.49243927001953\n",
      "Loss is 39.62996292114258\n",
      "Loss is 39.47174072265625\n",
      "Loss is 39.65428924560547\n",
      "Loss is 39.87224578857422\n",
      "Loss is 39.567588806152344\n",
      "Loss is 39.120426177978516\n",
      "Loss is 40.540924072265625\n",
      "Loss is 39.04827880859375\n",
      "Loss is 39.81692123413086\n",
      "Loss is 39.457298278808594\n",
      "Loss is 39.70067596435547\n",
      "Loss is 39.38643264770508\n",
      "Loss is 39.946441650390625\n",
      "Loss is 38.972991943359375\n",
      "Loss is 39.82944107055664\n",
      "Loss is 39.81665802001953\n",
      "Loss is 39.849029541015625\n",
      "Loss is 38.49924850463867\n",
      "Loss is 39.979156494140625\n",
      "Loss is 39.344139099121094\n",
      "Loss is 40.119537353515625\n",
      "Loss is 39.6599235534668\n",
      "Loss is 39.36012268066406\n",
      "Loss is 39.3952522277832\n",
      "Loss is 39.39412307739258\n",
      "Loss is 39.97224807739258\n",
      "Loss is 39.472015380859375\n",
      "Loss is 39.261741638183594\n",
      "Loss is 39.54346466064453\n",
      "Loss is 39.780330657958984\n",
      "Loss is 39.41157913208008\n",
      "Loss is 39.77170944213867\n",
      "Loss is 39.62734603881836\n",
      "Loss is 39.63200378417969\n",
      "Loss is 39.72501754760742\n",
      "Loss is 39.329280853271484\n",
      "Loss is 39.65621566772461\n",
      "Loss is 39.62933349609375\n",
      "Loss is 39.245330810546875\n",
      "Loss is 40.04244613647461\n",
      "Loss is 39.19734191894531\n",
      "Loss is 39.37138748168945\n",
      "Loss is 39.81540298461914\n",
      "Loss is 39.390071868896484\n",
      "Loss is 39.504844665527344\n",
      "Loss is 39.84654998779297\n",
      "Loss is 39.155052185058594\n",
      "Loss is 39.82720947265625\n",
      "Loss is 40.05591583251953\n",
      "Loss is 38.765193939208984\n",
      "Loss is 39.654396057128906\n",
      "Loss is 39.41932678222656\n",
      "Loss is 39.39588165283203\n",
      "Loss is 39.72652816772461\n",
      "Loss is 39.31462097167969\n",
      "Loss is 39.70382308959961\n",
      "Loss is 39.610496520996094\n",
      "Loss is 39.4649543762207\n",
      "Loss is 39.469703674316406\n",
      "Loss is 39.44683837890625\n",
      "Loss is 39.61458206176758\n",
      "Loss is 39.70659637451172\n",
      "Loss is 39.26150894165039\n",
      "Loss is 39.61656188964844\n",
      "Loss is 39.43056106567383\n",
      "Loss is 39.53511428833008\n",
      "Loss is 39.722023010253906\n",
      "Loss is 39.463111877441406\n",
      "Loss is 39.64395523071289\n",
      "Loss is 39.60940170288086\n",
      "Loss is 39.310550689697266\n",
      "Loss is 39.67534255981445\n",
      "Loss is 39.36140823364258\n",
      "Loss is 40.058570861816406\n",
      "Loss is 39.31898498535156\n",
      "Loss is 39.293270111083984\n",
      "Loss is 39.64600372314453\n",
      "Loss is 39.7802734375\n",
      "Loss is 39.299564361572266\n",
      "Loss is 39.384708404541016\n",
      "Loss is 39.71598434448242\n",
      "Loss is 39.83872604370117\n",
      "Loss is 39.59794998168945\n",
      "Loss is 39.18878936767578\n",
      "Loss is 39.88481521606445\n",
      "Loss is 39.46295928955078\n",
      "Loss is 38.91588592529297\n",
      "Loss is 39.53525924682617\n",
      "Loss is 39.70461654663086\n",
      "Loss is 39.242069244384766\n",
      "Loss is 39.367801666259766\n",
      "Loss is 39.64121627807617\n",
      "Loss is 39.51854705810547\n",
      "Loss is 39.57178497314453\n",
      "Loss is 39.76335906982422\n",
      "Loss is 39.14702606201172\n",
      "Loss is 39.621826171875\n",
      "Loss is 39.44098663330078\n",
      "Loss is 39.45528030395508\n",
      "Loss is 39.27851486206055\n",
      "Loss is 39.47005081176758\n",
      "Loss is 39.47255325317383\n",
      "Loss is 38.988983154296875\n",
      "Loss is 39.69419860839844\n",
      "Loss is 40.20805740356445\n",
      "Loss is 39.4068603515625\n",
      "Loss is 39.43964385986328\n",
      "Loss is 38.999149322509766\n",
      "Loss is 39.813907623291016\n",
      "Loss is 39.02674865722656\n",
      "Loss is 40.054176330566406\n",
      "Loss is 39.325477600097656\n",
      "Loss is 39.32570266723633\n",
      "Loss is 39.18811798095703\n",
      "Loss is 39.983123779296875\n",
      "Loss is 39.408851623535156\n",
      "Loss is 39.36637496948242\n",
      "Loss is 39.800296783447266\n",
      "Loss is 38.94586944580078\n",
      "Loss is 39.064613342285156\n",
      "Loss is 39.92247009277344\n",
      "Loss is 39.59503936767578\n",
      "Loss is 39.32330322265625\n",
      "Loss is 39.50822448730469\n",
      "Loss is 39.82674026489258\n",
      "Loss is 39.31497573852539\n",
      "Loss is 39.17652130126953\n",
      "Loss is 39.87919616699219\n",
      "Loss is 39.07801818847656\n",
      "Loss is 39.633113861083984\n",
      "Loss is 39.368202209472656\n",
      "Loss is 39.096160888671875\n",
      "Loss is 39.85533905029297\n",
      "Loss is 39.27597427368164\n",
      "Loss is 39.75089645385742\n",
      "Loss is 39.07515335083008\n",
      "Loss is 39.59993362426758\n",
      "Loss is 39.59487533569336\n",
      "Loss is 39.52705001831055\n",
      "Loss is 39.706329345703125\n",
      "Loss is 39.32959747314453\n",
      "Loss is 39.59303283691406\n",
      "Loss is 39.18781280517578\n",
      "Loss is 39.187522888183594\n",
      "Loss is 39.92116165161133\n",
      "Loss is 38.958030700683594\n",
      "Loss is 39.30734634399414\n",
      "Loss is 39.42556381225586\n",
      "Loss is 39.45697021484375\n",
      "Loss is 39.93753433227539\n",
      "Loss is 39.459957122802734\n",
      "Loss is 39.14763641357422\n",
      "Loss is 39.70046615600586\n",
      "Loss is 39.47917175292969\n",
      "Loss is 39.20589065551758\n",
      "Loss is 39.60329055786133\n",
      "Loss is 39.280799865722656\n",
      "Loss is 39.38444137573242\n",
      "Loss is 39.92005920410156\n",
      "Loss is 39.22740936279297\n",
      "Loss is 39.35472106933594\n",
      "Loss is 39.51197052001953\n",
      "Loss is 39.38185119628906\n",
      "Loss is 39.43370056152344\n",
      "Loss is 39.52347183227539\n",
      "Loss is 39.322357177734375\n",
      "Loss is 39.171871185302734\n",
      "Loss is 39.63910675048828\n",
      "Loss is 39.40757751464844\n",
      "Loss is 39.088218688964844\n",
      "Loss is 39.274078369140625\n",
      "Loss is 39.65847396850586\n",
      "Loss is 39.47833251953125\n",
      "Loss is 39.1280403137207\n",
      "Loss is 39.579524993896484\n",
      "Loss is 39.792503356933594\n",
      "Loss is 39.09391784667969\n",
      "Loss is 39.49845504760742\n",
      "Loss is 38.855777740478516\n",
      "Loss is 39.7182731628418\n",
      "Loss is 39.67735290527344\n",
      "Loss is 39.13737106323242\n",
      "Loss is 39.43375778198242\n",
      "Loss is 39.60118865966797\n",
      "Loss is 38.76485824584961\n",
      "Loss is 39.924644470214844\n",
      "Loss is 39.171852111816406\n",
      "Loss is 39.69648742675781\n",
      "Loss is 39.193336486816406\n",
      "Loss is 39.73540115356445\n",
      "Loss is 39.090641021728516\n",
      "Loss is 39.16242980957031\n",
      "Loss is 39.530052185058594\n",
      "Loss is 39.40448760986328\n",
      "Loss is 39.377647399902344\n",
      "Loss is 39.186100006103516\n",
      "Loss is 39.474552154541016\n",
      "Loss is 39.67308044433594\n",
      "Loss is 39.084938049316406\n",
      "Loss is 39.511985778808594\n",
      "Loss is 39.35131072998047\n",
      "Loss is 39.48163986206055\n",
      "Loss is 39.35847854614258\n",
      "Loss is 39.44235610961914\n",
      "Loss is 39.232208251953125\n",
      "Loss is 39.601593017578125\n",
      "Loss is 38.9216423034668\n",
      "Loss is 39.89276885986328\n",
      "Loss is 39.21207809448242\n",
      "Loss is 39.32975769042969\n",
      "Loss is 39.499664306640625\n",
      "Loss is 39.15808868408203\n",
      "Loss is 39.36311721801758\n",
      "Loss is 39.072200775146484\n",
      "Loss is 39.518001556396484\n",
      "Loss is 39.50425338745117\n",
      "Loss is 39.091400146484375\n",
      "Loss is 39.33710861206055\n",
      "Loss is 39.687679290771484\n",
      "Loss is 39.243507385253906\n",
      "Loss is 39.07862091064453\n",
      "Loss is 39.27936935424805\n",
      "Loss is 39.837890625\n",
      "Loss is 39.318668365478516\n",
      "Loss is 39.275001525878906\n",
      "Loss is 39.26410675048828\n",
      "Loss is 39.384925842285156\n",
      "Loss is 39.721500396728516\n",
      "Loss is 39.017173767089844\n",
      "Loss is 39.69718551635742\n",
      "Loss is 39.31437301635742\n",
      "Loss is 39.35002899169922\n",
      "Loss is 39.324031829833984\n",
      "Loss is 38.98665237426758\n",
      "Loss is 39.6945915222168\n",
      "Loss is 39.20935821533203\n",
      "Loss is 39.643272399902344\n",
      "Loss is 39.546016693115234\n",
      "Loss is 38.41780471801758\n",
      "Loss is 39.81189727783203\n",
      "Loss is 39.41288757324219\n",
      "Loss is 39.45486831665039\n",
      "Loss is 39.166072845458984\n",
      "Loss is 39.38872146606445\n",
      "Loss is 39.47911834716797\n",
      "Loss is 39.353240966796875\n",
      "Loss is 39.1888427734375\n",
      "Loss is 39.605690002441406\n",
      "Loss is 39.034393310546875\n",
      "Loss is 39.20999526977539\n",
      "Loss is 39.68650436401367\n",
      "Loss is 39.222469329833984\n",
      "Loss is 39.80306625366211\n",
      "Loss is 38.6628532409668\n",
      "Loss is 39.606781005859375\n",
      "Loss is 38.96131134033203\n",
      "Loss is 39.196311950683594\n",
      "Loss is 39.350337982177734\n",
      "Loss is 39.37120056152344\n",
      "Loss is 39.477149963378906\n",
      "Loss is 39.69681167602539\n",
      "Loss is 39.11778259277344\n",
      "Loss is 39.25095748901367\n",
      "Loss is 39.819488525390625\n",
      "Loss is 38.81475830078125\n",
      "Loss is 39.33357620239258\n",
      "Loss is 39.783470153808594\n",
      "Loss is 38.66645050048828\n",
      "Loss is 39.359153747558594\n",
      "Loss is 39.6763801574707\n",
      "Loss is 39.40933609008789\n",
      "Loss is 38.904903411865234\n",
      "Loss is 39.12274932861328\n",
      "Loss is 39.65087127685547\n",
      "Loss is 39.32592010498047\n",
      "Loss is 39.50108337402344\n",
      "Loss is 38.73679733276367\n",
      "Loss is 39.65921401977539\n",
      "Loss is 39.348793029785156\n",
      "Loss is 39.4753532409668\n",
      "Loss is 38.729156494140625\n",
      "Loss is 39.48949432373047\n",
      "Loss is 39.447021484375\n",
      "Loss is 39.38367462158203\n",
      "Loss is 39.23665237426758\n",
      "Loss is 39.35232925415039\n",
      "Loss is 39.31370162963867\n",
      "Loss is 38.7155647277832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is 39.40446090698242\n",
      "Loss is 39.182350158691406\n",
      "Loss is 39.92704391479492\n",
      "Loss is 38.75687789916992\n",
      "Loss is 39.31351089477539\n",
      "Loss is 39.35899353027344\n",
      "Loss is 39.30873107910156\n",
      "Loss is 39.43804168701172\n",
      "Loss is 39.07075119018555\n",
      "Loss is 38.9127197265625\n",
      "Loss is 39.432132720947266\n",
      "Loss is 39.15298080444336\n",
      "Loss is 39.442806243896484\n",
      "Loss is 39.33467102050781\n",
      "Loss is 39.22709274291992\n",
      "Loss is 39.13779830932617\n",
      "Loss is 39.054508209228516\n",
      "Loss is 40.03168869018555\n",
      "Loss is 38.64311599731445\n",
      "Loss is 39.50849533081055\n",
      "Loss is 38.761756896972656\n",
      "Loss is 39.96044158935547\n",
      "Loss is 38.768367767333984\n",
      "Loss is 39.53134536743164\n",
      "Loss is 39.246158599853516\n",
      "Loss is 39.15071487426758\n",
      "Loss is 39.05927658081055\n",
      "Loss is 39.15628433227539\n",
      "Loss is 39.89268112182617\n",
      "Loss is 39.01598358154297\n",
      "Loss is 38.89332962036133\n",
      "Loss is 39.615631103515625\n",
      "Loss is 38.965335845947266\n",
      "Loss is 39.47858810424805\n",
      "Loss is 39.04080581665039\n",
      "Loss is 39.37290954589844\n",
      "Loss is 39.014835357666016\n",
      "Loss is 39.169681549072266\n",
      "Loss is 39.299713134765625\n",
      "Loss is 38.64731216430664\n",
      "Loss is 39.31588363647461\n",
      "Loss is 39.649967193603516\n",
      "Loss is 38.889713287353516\n",
      "Loss is 39.65015411376953\n",
      "Loss is 39.120216369628906\n",
      "Loss is 39.63328552246094\n",
      "Loss is 38.65932846069336\n",
      "Loss is 39.236392974853516\n",
      "Loss is 39.84202194213867\n",
      "Loss is 38.76375961303711\n",
      "Loss is 39.5102653503418\n",
      "Loss is 39.13511276245117\n",
      "Loss is 38.838623046875\n",
      "Loss is 39.009483337402344\n",
      "Loss is 39.28887176513672\n",
      "Loss is 39.246795654296875\n",
      "Loss is 39.425872802734375\n",
      "Loss is 38.958683013916016\n",
      "Loss is 39.1762809753418\n",
      "Loss is 38.92613220214844\n",
      "Loss is 39.480289459228516\n",
      "Loss is 39.55147171020508\n",
      "Loss is 38.83992385864258\n",
      "Loss is 39.443973541259766\n",
      "Loss is 39.11483383178711\n",
      "Loss is 38.93334197998047\n",
      "Loss is 39.777618408203125\n",
      "Loss is 38.815711975097656\n",
      "Loss is 39.084651947021484\n",
      "Loss is 38.86717987060547\n",
      "Loss is 40.08549499511719\n",
      "Loss is 38.3870964050293\n",
      "Loss is 39.61326599121094\n",
      "Loss is 38.865196228027344\n",
      "Loss is 39.60346603393555\n",
      "Loss is 38.8982048034668\n",
      "Loss is 39.451393127441406\n",
      "Loss is 39.01188278198242\n",
      "Loss is 39.101497650146484\n",
      "Loss is 39.12519836425781\n",
      "Loss is 39.151248931884766\n",
      "Loss is 39.1863899230957\n",
      "Loss is 39.21088790893555\n",
      "Loss is 39.50867462158203\n",
      "Loss is 38.89274215698242\n",
      "Loss is 39.047672271728516\n",
      "Loss is 39.21443176269531\n",
      "Loss is 38.94263458251953\n",
      "Loss is 39.3762092590332\n",
      "Loss is 38.97686767578125\n",
      "Loss is 39.299041748046875\n",
      "Loss is 39.18622970581055\n",
      "Loss is 39.08407211303711\n",
      "Loss is 39.38780212402344\n",
      "Loss is 39.35126876831055\n",
      "Loss is 39.00933074951172\n",
      "Loss is 38.93716812133789\n",
      "Loss is 39.91181564331055\n",
      "Loss is 38.52511215209961\n",
      "Loss is 39.385562896728516\n",
      "Loss is 39.28980255126953\n",
      "Loss is 39.07780075073242\n",
      "Loss is 39.06794738769531\n",
      "Loss is 39.360931396484375\n",
      "Loss is 38.64870071411133\n",
      "Loss is 39.61745071411133\n",
      "Loss is 39.2056884765625\n",
      "Loss is 38.75911331176758\n",
      "Loss is 39.91057586669922\n",
      "Loss is 38.736141204833984\n",
      "Loss is 39.45420837402344\n",
      "Loss is 39.14515686035156\n",
      "Loss is 39.04403305053711\n",
      "Loss is 39.05778121948242\n",
      "Loss is 39.406394958496094\n",
      "Loss is 38.999427795410156\n",
      "Loss is 38.46680450439453\n",
      "Loss is 39.371952056884766\n",
      "Loss is 38.82785415649414\n",
      "Loss is 40.08891677856445\n",
      "Loss is 38.409759521484375\n",
      "Loss is 39.349952697753906\n",
      "Loss is 39.337371826171875\n",
      "Loss is 38.6047477722168\n",
      "Loss is 39.21778869628906\n",
      "Loss is 39.53971481323242\n",
      "Loss is 39.30354309082031\n",
      "Loss is 39.151573181152344\n",
      "Loss is 38.28392791748047\n",
      "Loss is 39.74695587158203\n",
      "Loss is 39.02873229980469\n",
      "Loss is 39.13727569580078\n",
      "Loss is 39.42494201660156\n",
      "Loss is 39.09999084472656\n",
      "Loss is 39.1823616027832\n",
      "Loss is 39.005794525146484\n",
      "Loss is 39.31428909301758\n",
      "Loss is 38.7444953918457\n",
      "Loss is 39.083831787109375\n",
      "Loss is 39.16352462768555\n",
      "Loss is 39.07899856567383\n",
      "Loss is 39.404762268066406\n",
      "Loss is 39.15669250488281\n",
      "Loss is 38.56003952026367\n",
      "Loss is 39.11186599731445\n",
      "Loss is 39.1779899597168\n",
      "Loss is 39.379310607910156\n",
      "Loss is 39.213558197021484\n",
      "Loss is 38.985008239746094\n",
      "Loss is 38.9019660949707\n",
      "Loss is 39.632102966308594\n",
      "Loss is 38.68058395385742\n",
      "Loss is 39.15848159790039\n",
      "Loss is 39.313167572021484\n",
      "Loss is 38.57563781738281\n",
      "Loss is 39.25395965576172\n",
      "Loss is 39.296722412109375\n",
      "Loss is 38.83806610107422\n",
      "Loss is 39.52615737915039\n",
      "Loss is 39.155006408691406\n",
      "Loss is 38.784873962402344\n",
      "Loss is 39.13842010498047\n",
      "Loss is 38.800697326660156\n",
      "Loss is 39.269554138183594\n",
      "Loss is 39.622135162353516\n",
      "Loss is 38.83159255981445\n",
      "Loss is 39.14535903930664\n",
      "Loss is 38.94723129272461\n",
      "Loss is 39.042518615722656\n",
      "Loss is 38.86418151855469\n",
      "Loss is 39.133811950683594\n",
      "Loss is 38.9966926574707\n",
      "Loss is 39.14990997314453\n",
      "Loss is 39.03247833251953\n",
      "Loss is 39.25478744506836\n",
      "Loss is 39.15949249267578\n",
      "Loss is 39.134864807128906\n",
      "Loss is 38.893775939941406\n",
      "Loss is 39.02690505981445\n",
      "Loss is 39.22173309326172\n",
      "Loss is 38.94365310668945\n",
      "Loss is 39.12017059326172\n",
      "Loss is 39.07844543457031\n",
      "Loss is 39.250160217285156\n",
      "Loss is 38.98189926147461\n",
      "Loss is 39.24159622192383\n",
      "Loss is 38.76205825805664\n",
      "Loss is 39.25545883178711\n",
      "Loss is 39.20022964477539\n",
      "Loss is 38.99939727783203\n",
      "Loss is 38.89360046386719\n",
      "Loss is 39.24555587768555\n",
      "Loss is 39.07276153564453\n",
      "Loss is 38.745628356933594\n",
      "Loss is 39.48994827270508\n",
      "Loss is 39.00139236450195\n",
      "Loss is 38.998600006103516\n",
      "Loss is 38.96023178100586\n",
      "Loss is 39.30167770385742\n",
      "Loss is 39.091758728027344\n",
      "Loss is 38.76866149902344\n",
      "Loss is 39.07059860229492\n",
      "Loss is 39.119720458984375\n",
      "Loss is 39.37901306152344\n",
      "Loss is 38.74549865722656\n",
      "Loss is 39.12876892089844\n",
      "Loss is 39.5528678894043\n",
      "Loss is 38.87744903564453\n",
      "Loss is 39.21205139160156\n",
      "Loss is 39.04819869995117\n",
      "Loss is 38.64256286621094\n",
      "Loss is 38.8736572265625\n",
      "Loss is 39.60670471191406\n",
      "Loss is 38.772125244140625\n",
      "Loss is 39.01762771606445\n",
      "Loss is 39.07334899902344\n",
      "Loss is 38.94496154785156\n",
      "Loss is 38.80332565307617\n",
      "Loss is 39.26778030395508\n",
      "Loss is 38.56412887573242\n",
      "Loss is 39.450199127197266\n",
      "Loss is 38.58580017089844\n",
      "Loss is 39.769535064697266\n",
      "Loss is 38.63161849975586\n",
      "Loss is 39.096527099609375\n",
      "Loss is 39.176753997802734\n",
      "Loss is 38.747249603271484\n",
      "Loss is 39.19581985473633\n",
      "Loss is 38.917171478271484\n",
      "Loss is 38.82145309448242\n",
      "Loss is 39.503509521484375\n",
      "Loss is 38.84292221069336\n",
      "Loss is 38.635765075683594\n",
      "Loss is 39.14133071899414\n",
      "Loss is 38.9123649597168\n",
      "Loss is 39.29246520996094\n",
      "Loss is 38.97651672363281\n",
      "Loss is 39.08381271362305\n",
      "Loss is 38.75271224975586\n",
      "Loss is 39.3177490234375\n",
      "Loss is 38.976993560791016\n",
      "Loss is 39.0076904296875\n",
      "Loss is 39.136531829833984\n",
      "Loss is 39.100341796875\n",
      "Loss is 38.994476318359375\n",
      "Loss is 39.07472610473633\n",
      "Loss is 39.34356689453125\n",
      "Loss is 38.771385192871094\n",
      "Loss is 39.26496887207031\n",
      "Loss is 39.48094940185547\n",
      "Loss is 38.459869384765625\n",
      "Loss is 39.193443298339844\n",
      "Loss is 38.66755676269531\n",
      "Loss is 38.840694427490234\n",
      "Loss is 38.97959518432617\n",
      "Loss is 39.07321548461914\n",
      "Loss is 39.16633987426758\n",
      "Loss is 38.82001876831055\n",
      "Loss is 39.18900680541992\n",
      "Loss is 38.84885787963867\n",
      "Loss is 39.34522247314453\n",
      "Loss is 38.488441467285156\n",
      "Loss is 39.130428314208984\n",
      "Loss is 39.248870849609375\n",
      "Loss is 39.08729934692383\n",
      "Loss is 39.236236572265625\n",
      "Loss is 38.767608642578125\n",
      "Loss is 38.86524963378906\n",
      "Loss is 39.487571716308594\n",
      "Loss is 38.857059478759766\n",
      "Loss is 39.226409912109375\n",
      "Loss is 38.95424270629883\n",
      "Loss is 39.13019943237305\n",
      "Loss is 38.6338005065918\n",
      "Loss is 39.11991882324219\n",
      "Loss is 38.60375213623047\n",
      "Loss is 39.63393783569336\n",
      "Loss is 38.7264404296875\n",
      "Loss is 38.532379150390625\n",
      "Loss is 39.109352111816406\n",
      "Loss is 39.352359771728516\n",
      "Loss is 38.780113220214844\n",
      "Loss is 39.04416275024414\n",
      "Loss is 38.84252166748047\n",
      "Loss is 39.0811767578125\n",
      "Loss is 38.72140121459961\n",
      "Loss is 39.26213455200195\n",
      "Loss is 38.92098617553711\n",
      "Loss is 39.13761901855469\n",
      "Loss is 38.79658889770508\n",
      "Loss is 38.51688003540039\n",
      "Loss is 39.2295036315918\n",
      "Loss is 38.95512771606445\n",
      "Loss is 39.130680084228516\n",
      "Loss is 39.157012939453125\n",
      "Loss is 39.08565902709961\n",
      "Loss is 38.127464294433594\n",
      "Loss is 39.27501678466797\n",
      "Loss is 38.93102264404297\n",
      "Loss is 38.57744598388672\n",
      "Loss is 39.41586685180664\n",
      "Loss is 38.98127746582031\n",
      "Loss is 39.346168518066406\n",
      "Loss is 38.66615295410156\n",
      "Loss is 38.962406158447266\n",
      "Loss is 38.95825958251953\n",
      "Loss is 39.36335372924805\n",
      "Loss is 38.411598205566406\n",
      "Loss is 39.25748825073242\n",
      "Loss is 39.0299186706543\n",
      "Loss is 38.651241302490234\n",
      "Loss is 39.01830291748047\n",
      "Loss is 38.94694900512695\n",
      "Loss is 39.04320526123047\n",
      "Loss is 38.89311599731445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is 39.15685272216797\n",
      "Loss is 38.491241455078125\n",
      "Loss is 38.88188171386719\n",
      "Loss is 39.051239013671875\n",
      "Loss is 38.782169342041016\n",
      "Loss is 38.70978546142578\n",
      "Loss is 39.13188552856445\n",
      "Loss is 39.137603759765625\n",
      "Loss is 38.87211608886719\n",
      "Loss is 38.99895477294922\n",
      "Loss is 38.467063903808594\n",
      "Loss is 38.9178466796875\n",
      "Loss is 39.29508972167969\n",
      "Loss is 38.740699768066406\n",
      "Loss is 38.890926361083984\n",
      "Loss is 38.6588249206543\n",
      "Loss is 39.27604293823242\n",
      "Loss is 39.026432037353516\n",
      "Loss is 38.77635955810547\n",
      "Loss is 39.020050048828125\n",
      "Loss is 38.90385055541992\n",
      "Loss is 38.69395446777344\n",
      "Loss is 39.28723907470703\n",
      "Loss is 38.88602828979492\n",
      "Loss is 38.60912322998047\n",
      "Loss is 39.15458297729492\n",
      "Loss is 38.28508377075195\n",
      "Loss is 39.48484802246094\n",
      "Loss is 38.66511917114258\n",
      "Loss is 38.60062789916992\n",
      "Loss is 39.04462814331055\n",
      "Loss is 38.8216552734375\n",
      "Loss is 38.96780776977539\n",
      "Loss is 38.362972259521484\n",
      "Loss is 39.38046646118164\n",
      "Loss is 38.73121643066406\n",
      "Loss is 38.847835540771484\n",
      "Loss is 38.66238784790039\n",
      "Loss is 38.94186019897461\n",
      "Loss is 38.66001510620117\n",
      "Loss is 39.28527069091797\n",
      "Loss is 39.2120361328125\n",
      "Loss is 38.58663558959961\n",
      "Loss is 38.69157028198242\n",
      "Loss is 39.32080841064453\n",
      "Loss is 38.87340545654297\n",
      "Loss is 38.83306884765625\n",
      "Loss is 38.87136459350586\n",
      "Loss is 38.58665466308594\n",
      "Loss is 39.29545974731445\n",
      "Loss is 38.92812728881836\n",
      "Loss is 38.844459533691406\n",
      "Loss is 38.759307861328125\n",
      "Loss is 38.773521423339844\n",
      "Loss is 38.81084442138672\n",
      "Loss is 39.0216064453125\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv( 'cars.csv' )\n",
    "data.head()\n",
    "\n",
    "continuous_features = data[ [ \"Identification.Year\",\"Engine Information.Engine Statistics.Horsepower\",\"Engine Information.Engine Statistics.Torque\"] ].values / 100 \n",
    "#categorical_research_features = data[ [ 'Research' ] ].values \n",
    "\n",
    "X = np.concatenate( [ continuous_features ] , axis=1 )\n",
    "Y = data[ [ 'Fuel Information.City mpg' ] ].values\n",
    "\n",
    "train_features , test_features ,train_labels, test_labels = train_test_split( X , Y , test_size=0.2 )\n",
    "\n",
    "X = tf.constant( train_features , dtype=tf.float32 )\n",
    "Y = tf.constant( train_labels , dtype=tf.float32 ) \n",
    "                                                          \n",
    "test_X = tf.constant( test_features , dtype=tf.float32 ) \n",
    "test_Y = tf.constant( test_labels , dtype=tf.float32 ) \n",
    "\n",
    "def mean_squared_error( Y , y_pred ):\n",
    "    return tf.reduce_mean( tf.square( y_pred - Y ) )\n",
    "# Analytic gradient of the mean squared error:\n",
    "def mean_squared_error_deriv( Y , y_pred ):\n",
    "    return tf.reshape( tf.reduce_mean( 2 * ( y_pred - Y ) ) , [ 1 , 1 ] )    \n",
    "\n",
    "# Generate approximation of the derivative using backpropogration:\n",
    "def mean_squared_error_deriv( Y , y_pred ):\n",
    "    return tf.reshape( tf.reduce_mean( 2 * ( y_pred - Y ) ) , [ 1 , 1 ] )    \n",
    "\n",
    "# Apply matrix multiplication operation to the vector of betas. Add bias term instead of creating an additional row of the design matrix.\n",
    "def h ( X , weights , bias ):\n",
    "    return tf.tensordot( X , weights , axes=1 ) + bias\n",
    "\n",
    "# Arbitrary choices. Note to self: How to optimize these for performance/convergence?\n",
    "num_epochs = 1000\n",
    "num_samples = X.shape[0]\n",
    "batch_size = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "# The data.Dataset call below makes the data available within Tensorflow and allows for transformations to be applied to it.\n",
    "dataset = tf.data.Dataset.from_tensor_slices(( X , Y )) \n",
    "dataset = dataset.shuffle( 500 ).repeat( num_epochs ).batch( batch_size )\n",
    "iterator = dataset.__iter__()\n",
    "\n",
    "num_features = X.shape[1]\n",
    "weights = tf.random.normal( ( num_features , 1 ) )\n",
    "bias = 0\n",
    "\n",
    "epochs_plot = list()\n",
    "loss_plot = list()\n",
    "\n",
    "for i in range( num_epochs ) :\n",
    "    \n",
    "    epoch_loss = list()\n",
    "    for b in range( int(num_samples/batch_size) ):\n",
    "        x_batch , y_batch = iterator.get_next()\n",
    "   \n",
    "        output = h( x_batch , weights , bias ) \n",
    "        loss = epoch_loss.append( mean_squared_error( y_batch , output ).numpy() )\n",
    "    \n",
    "        dJ_dH = mean_squared_error_deriv( y_batch , output)\n",
    "        dH_dW = x_batch\n",
    "        dJ_dW = tf.reduce_mean( dJ_dH * dH_dW )\n",
    "        dJ_dB = tf.reduce_mean( dJ_dH )\n",
    "    \n",
    "        weights -= ( learning_rate * dJ_dW )\n",
    "        bias -= ( learning_rate * dJ_dB ) \n",
    "        \n",
    "    loss = np.array( epoch_loss ).mean()\n",
    "    epochs_plot.append( i + 1 )\n",
    "    loss_plot.append( loss ) \n",
    "    \n",
    "    print( 'Loss is {}'.format( loss ) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot( epochs_plot , loss_plot ) \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
