{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Exercises: Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scope: \n",
    "Use Tensorflow 2.0 and pandas to build a simple linear regression model and then test the results on a hold-out set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Optimization\n",
    "\n",
    "To \"warm-up\", I'll use the tensorflow framework to solve a simple optimization problem. This will be done with the analytic gradient, and the autodifferentiation procedure which is standard in Tensorflow.\n",
    "\n",
    "First, define the function we will be using which is the Beale function:\n",
    "\n",
    "$$ f(x,y) = (1.5-x+xy)^2 + (2.25-x+xy^2)^2 + (2.625-x+xy^3)^2$$ \n",
    "\n",
    "The gradient is:\n",
    "\n",
    "$$  \\nabla f(x,y) =  \\begin{bmatrix}\n",
    "2 x (y^6 + y^4 - 2 y^3 - y^2 - 2 y + 3) + 5.25 y^3 + 4.5 y^2 + 3 y - 12.75 \\\\\n",
    " 6 x (x (y^5 + 0.666667 y^3 - y^2 - 0.333333 y - 0.333333) + 2.625 y^2 + 1.5 y + 0.5)\n",
    "\\end{bmatrix}  $$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_beale(x):\n",
    "    return (tf.convert_to_tensor( (1.5-x[0]+x[0]*x[1])**2 + (2.25-x[0]+x[0]*x[1]**2)**2 + (2.625-x[0]+x[0]*x[1]**3)**2) )\n",
    "\n",
    "def grad_beal(x):\n",
    "    dx = 2*x[0]*(x[1]**6 + x[1]**4 - 2*x[1]**3 - x[1]**2-2*x[1] + 3) + 5.25*x[1]**3+4.5*x[1]**2+3*x[1]-12.75\n",
    "    dy = 6*x[0]*(x[0]*(x[1]**5 + 2.0/3.0 * x[1]**3-x[1]**2-1.0/3.0 * x[1]-1.0/3.0) + 2.625*x[1]**2+1.5*x[1]+.5)\n",
    "    return(tf.convert_to_tensor([dx,dy]))\n",
    "\n",
    "def AD_beale(x):\n",
    "    x = tf.Variable(x,dtype = tf.float32) \n",
    "    with tf.GradientTape(persistent = True) as dv:\n",
    "        temp_beale = f_beale(x)\n",
    "    dx = dv.gradient(temp_beale, x)\n",
    "    return(dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see below that the results end up being the same for the derivatives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=bool, numpy=array([ True,  True])>"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_beal([1,1]) == AD_beale([1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, we will attemp to solve the problem using stochastic gradient descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(2,) dtype=float32, numpy=array([2.9961617 , 0.49904037], dtype=float32)>"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt = tf.keras.optimizers.SGD(learning_rate = .01)\n",
    "#opt = tf.keras.optimizers.Adam(learning_rate = .01)\n",
    "x_0 = tf.Variable([1.0,1.0],dtype = tf.float32)\n",
    "func_for_opt = lambda: f_beale(x_0)\n",
    "\n",
    "for epoch in range(1500):\n",
    "    opt.minimize(func_for_opt, [x_0]).numpy()\n",
    "\n",
    "x_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([156. 900. 132.], shape=(3,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x_vals = tf.Variable([1.0,2.0,3.0],name = \"x_vals\")\n",
    "x_vals_two = tf.Variable([2.0,3.0,4.0],name = \"x_vals_two\")\n",
    "with tf.GradientTape() as tape:\n",
    "    y = x_vals[0]**3+x_vals[2]*x_vals[1]**2 + 2*x_vals[2]+100\n",
    "    z_one = x_vals[1]*(4*(y) + x_vals[0]**2)    \n",
    "    y_two = x_vals_two[0]**3+x_vals_two[2]*x_vals_two[1]**2 + 2*x_vals_two[2]+100\n",
    "    z_two = x_vals_two[1]*(4*(y_two) + x_vals_two[0]**2)\n",
    "    z = z_one + z_two\n",
    "    \n",
    "a, b = tape.gradient(z , [z,x_vals_two]) \n",
    "print(b)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constrained Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next solve a constrained optimization problem which is a constrained version of the Rosenbrock banana function:\n",
    "\n",
    "$$ \\min_{x,y} f(x,y)  =  (1-x)^2 + 100(y-x^2)$$\n",
    "\n",
    "Subject to:\n",
    "$$ (x-1)^3 - y + 1 <=0 $$\n",
    "\n",
    "and:\n",
    "\n",
    "$$ x+y-2<=0 $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_rosenbrock(vals):\n",
    "    return(tf.convert_to_tensor(  (1-vals[0])**2 + 100*(vals[1]-vals[0]**2)  ) )\n",
    "\n",
    "def const1_rosenbrock(vals):    \n",
    "    return(tf.convert_to_tensor( 1.0*((vals[0]-vals[1])**3 - vals[1] + 1)   ))\n",
    "\n",
    "def const2_rosenbrock(vals):    \n",
    "    return(tf.convert_to_tensor( 1.0*(vals[0]+vals[1]-2)   ))\n",
    "\n",
    "def rosenbrock_lagrange(vals):\n",
    "    return(  f_rosenbrock(vals)+const1_rosenbrock(vals)+const2_rosenbrock(vals)   )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=-299.0>"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_rosenbrock([2.0,1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.SGD(learning_rate = 1e-7,momentum = .9)\n",
    "#opt = tf.keras.optimizers.Adam(learning_rate = 5e-6)\n",
    "x_0 = tf.Variable([.1,.1 ],dtype = tf.float32)\n",
    "func_for_opt = lambda: tf.abs(f_rosenbrock(x_0))\n",
    "\n",
    "\n",
    "#for epoch in range(1000):\n",
    "i=1\n",
    "while (func_for_opt().numpy()>1e-4)&(i < 60000):\n",
    "#    print(i)\n",
    "    if i%1000 == 0:\n",
    "        print(\"At iteration \" + str(i) +  \" current results are \" + str(x_0.numpy() )+ \" with function value of \"+str( func_for_opt().numpy())  + \".\")\n",
    "    opt.minimize(func_for_opt, [x_0]).numpy()\n",
    "    i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 5000 current results are [1.0583223 1.119438 ] with function value of 0.0006868541.\n",
      "At iteration 10000 current results are [1.0583223 1.1194379] with function value of 0.000674814.\n",
      "At iteration 15000 current results are [1.0583256 1.1194365] with function value of 0.00016772747.\n",
      "At iteration 20000 current results are [1.058328  1.1194353] with function value of 0.00078471005.\n",
      "At iteration 25000 current results are [1.0583247 1.1194369] with function value of 6.9633126e-05.\n",
      "At iteration 30000 current results are [1.0583297 1.1194346] with function value of 0.0012120008.\n",
      "At iteration 35000 current results are [1.0583258 1.1194364] with function value of 0.00020335615.\n",
      "At iteration 40000 current results are [1.0583266 1.119436 ] with function value of 0.0004169941.\n",
      "At iteration 45000 current results are [1.058322  1.1194382] with function value of 0.0007818341.\n",
      "At iteration 50000 current results are [1.0583224 1.1194379] with function value of 0.00065122545.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-307-bc2f5c0a9244>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m5000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"At iteration \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m  \u001b[0;34m\" current results are \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0;34m\" with function value of \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mfunc_for_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;34m+\u001b[0m \u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_for_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx_0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mi\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(self, loss, var_list, grad_loss, name)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m     \"\"\"\n\u001b[0;32m--> 374\u001b[0;31m     grads_and_vars = self._compute_gradients(\n\u001b[0m\u001b[1;32m    375\u001b[0m         loss, var_list=var_list, grad_loss=grad_loss)\n\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36m_compute_gradients\u001b[0;34m(self, loss, var_list, grad_loss)\u001b[0m\n\u001b[1;32m    432\u001b[0m     \u001b[0mvar_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/gradients\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m       \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m       \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clip_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1065\u001b[0m                           for x in nest.flatten(output_gradients)]\n\u001b[1;32m   1066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1067\u001b[0;31m     flat_grad = imperative_grad.imperative_grad(\n\u001b[0m\u001b[1;32m   1068\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1069\u001b[0m         \u001b[0mflat_targets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \"Unknown value for unconnected_gradients: %r\" % unconnected_gradients)\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m   return pywrap_tfe.TFE_Py_TapeGradient(\n\u001b[0m\u001b[1;32m     72\u001b[0m       \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m       \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mThe\u001b[0m \u001b[0mgradients\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mrespect\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minputs\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m   \"\"\"\n\u001b[0;32m--> 150\u001b[0;31m   \u001b[0mmock_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_MockOp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr_tuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_input_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m   \u001b[0mgrad_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_registry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_name\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mgrad_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, attrs, inputs, outputs, typ, skip_input_indices)\u001b[0m\n\u001b[1;32m    107\u001b[0m   \u001b[0;34m\"\"\"Pretends to be a tf.Operation for the gradient functions.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_input_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "opt = tf.keras.optimizers.SGD(learning_rate = 1e-8,momentum = .9)\n",
    "#opt = tf.keras.optimizers.Adam(learning_rate = 1e-6)\n",
    "x_0 = tf.Variable([1.1,1.1  ],dtype = tf.float32)\n",
    "func_for_opt = lambda: tf.abs(rosenbrock_lagrange(x_0))\n",
    "\n",
    "\n",
    "#for epoch in range(1000):\n",
    "i=1\n",
    "while (func_for_opt().numpy()>1e-6)&(i < 100000):\n",
    "#    print(i)\n",
    "    if i%5000 == 0:\n",
    "        print(\"At iteration \" + str(i) +  \" current results are \" + str(x_0.numpy() )+ \" with function value of \"+str( func_for_opt().numpy())  + \".\")\n",
    "    opt.minimize(func_for_opt, [x_0]).numpy()\n",
    "    i+=1\n",
    "\n",
    "print(\"Final results at iteration \" + str(i) +  \" where the final results are \" + str(x_0.numpy() )+ \" with function value of \"+str( func_for_opt().numpy())  + \".\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data/Format Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results above show the basic steps of working through a mathematical optimization problem. At this point, we pivot to the use of the mathematical optimization framework to solve a statistical problem. \n",
    "\n",
    "Simple linear regression uses the mean squared error function to find a linear relationship between a set of features and an outcome. This is defined as:\n",
    "\n",
    "$$ MSE(\\beta) = \\frac{\\sum_{i=1}^N (y_i - \\bar{\\beta}X_i )^2 }{N}$$\n",
    "\n",
    "Here, we step through a simple example of linear regression using tensorflow and an implementation of the MSE function.\n",
    "\n",
    "The functions are defined in the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(y_val ,x_val,weights):\n",
    "    output = tf.tensordot(X, weights, axes=1 ) \n",
    "    mse_val =  tf.reduce_mean( tf.square( output - y_val ) )\n",
    "    return(mse_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this numerical example, we'll use the cars data-set and fit a few different variables to predict the gas mileage. \n",
    "\n",
    "First step: Load the data into the environment and form the design matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 100 function value is 67.86177.\n",
      "At iteration 200 function value is 43.66971.\n",
      "At iteration 300 function value is 30.864977.\n",
      "At iteration 400 function value is 24.923626.\n",
      "At iteration 500 function value is 22.539116.\n",
      "At iteration 600 function value is 21.708658.\n",
      "At iteration 700 function value is 21.443983.\n",
      "At iteration 800 function value is 21.349493.\n",
      "At iteration 900 function value is 21.296299.\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv( 'cars.csv' )\n",
    "continuous_features = data[ [ \"Identification.Year\",\"Engine Information.Engine Statistics.Horsepower\",\"Engine Information.Engine Statistics.Torque\"] ].values / 100 \n",
    "X = np.concatenate( [ continuous_features ] , axis=1 )\n",
    "X = np.append(np.ones((X.shape[0],1) ) , X, axis=1)\n",
    "Y = data[ [ 'Fuel Information.City mpg' ] ].values\n",
    "\n",
    "# Perform basic subset selection in the code:\n",
    "train_features , test_features ,train_labels, test_labels = train_test_split( X , Y , test_size=0.2 )\n",
    "# Training data.\n",
    "X = tf.Variable( train_features , dtype=tf.float32 )\n",
    "Y = tf.Variable( train_labels , dtype=tf.float32 )                                                         \n",
    "# Testing data\n",
    "test_X = tf.Variable( test_features , dtype=tf.float32 ) \n",
    "test_Y = tf.Variable( test_labels , dtype=tf.float32 ) \n",
    "num_features = X.shape[1]\n",
    "# Define the coefficientst that we'll be starting with:\n",
    "#weights = tf.Variable(tf.random.normal((num_features,1)))\n",
    "weights = tf.Variable(tf.ones(4))\n",
    "\n",
    "def MSE(y_val ,x_val,weights):\n",
    "    output = tf.tensordot(X, weights, axes=1 )\n",
    "    mse_val =  tf.reduce_mean( tf.square( output - y_val ) )\n",
    "    return(mse_val)\n",
    "\n",
    "#mse_opt = tf.keras.optimizers.SGD(.001)\n",
    "mse_opt = tf.keras.optimizers.Adam(.001)\n",
    "weight_vals = weights\n",
    "def temp_mse(weight_vals):\n",
    "    return(MSE(Y,X,weight_vals))\n",
    "func_for_opt = lambda: tf.abs(temp_mse(weight_vals))\n",
    "mse_opt.minimize(func_for_opt, [weight_vals]).numpy()\n",
    "\n",
    "i=1\n",
    "while (temp_mse(weight_vals).numpy()>1e-2)&(i < 1000):\n",
    "#    print(i)\n",
    "    if i%100 == 0:\n",
    "        print(\"At iteration \" + str(i) +  \" function value is \"+str( func_for_opt().numpy())  + \".\")\n",
    "    mse_opt.minimize(func_for_opt, [weight_vals]).numpy()    \n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two Stage Regression\n",
    "\n",
    "One type of problem that's observed in econometrics is using the residuals from one model as the input to another model. This can be thought of as a constrained optimization problem where we try to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "# Analytic gradient of the mean squared error:\n",
    "def mean_squared_error_deriv( Y , y_pred ):\n",
    "    return tf.reshape( tf.reduce_mean( 2 * ( y_pred - Y ) ) , [ 1 , 1 ] )    \n",
    "\n",
    "# Generate approximation of the derivative using backpropogration:\n",
    " \n",
    "# Apply matrix multiplication operation to the vector of betas. Add bias term instead of creating an additional row of the design matrix.\n",
    "def h ( X , weights , bias ):\n",
    "    return tf.tensordot( X , weights , axes=1 ) + bias\n",
    "\n",
    "# Arbitrary choices. Note to self: How to optimize these for performance/convergence?\n",
    "num_epochs = 10\n",
    "num_samples = X.shape[0]\n",
    "batch_size = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "# The data.Dataset call below makes the data available within Tensorflow and allows for transformations to be applied to it.\n",
    "dataset = tf.data.Dataset.from_tensor_slices(( X , Y )) \n",
    "dataset = dataset.shuffle( 500 ).repeat( num_epochs ).batch( batch_size )\n",
    "iterator = dataset.__iter__()\n",
    "\n",
    "num_features = X.shape[1]\n",
    "weights = tf.random.normal( ( num_features , 1 ) )\n",
    "bias = 0\n",
    "\n",
    "epochs_plot = list()\n",
    "loss_plot = list()\n",
    "\n",
    "for i in range( num_epochs ) :\n",
    "    \n",
    "    epoch_loss = list()\n",
    "    for b in range( int(num_samples/batch_size) ):\n",
    "        x_batch , y_batch = iterator.get_next()\n",
    "   \n",
    "        output = h( x_batch , weights , bias ) \n",
    "        loss = epoch_loss.append( mean_squared_error( y_batch , output ).numpy() )\n",
    "    \n",
    "    \n",
    "        output = tf.Variable( output, dtype=tf.float32 ) \n",
    "        y_batch = tf.Variable( y_batch, dtype=tf.float32 ) \n",
    "        with tf.GradientTape() as tape:\n",
    "            mse_val = tf.reshape( tf.reduce_mean( tf.square( output - y_batch ) )  , [ 1 , 1 ] )    \n",
    "    \n",
    "        dJ_dH = tape.gradient(mse_val,output)\n",
    "    \n",
    "  #      dJ_dH = mean_squared_error_deriv( y_batch , output)\n",
    "#        print(dJ_dH)\n",
    "        \n",
    "        \n",
    "        dH_dW = x_batch\n",
    "        dJ_dW = tf.reduce_mean( dJ_dH * dH_dW )\n",
    "        dJ_dB = tf.reduce_mean( dJ_dH )\n",
    "    \n",
    "        weights -= ( learning_rate * dJ_dW )\n",
    "        bias -= ( learning_rate * dJ_dB ) \n",
    "        \n",
    "    loss = np.array( epoch_loss ).mean()\n",
    "    epochs_plot.append( i + 1 )\n",
    "    loss_plot.append( loss ) \n",
    "    \n",
    "    print( 'Loss is {}'.format( loss ) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(5076, 1) dtype=float32, numpy=\n",
       "array([[-21.167173],\n",
       "       [-20.933979],\n",
       "       [-20.933979],\n",
       "       ...,\n",
       "       [-22.604477],\n",
       "       [-21.499138],\n",
       "       [-21.499138]], dtype=float32)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[77.11303]], dtype=float32)>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error_deriv(output_1,test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the calculation of the function goes in two main steps:\n",
    "1. Calculate the hat matrix using the h() function.\n",
    "2. Use the h() results (output) to produce y_pred.\n",
    "3. Use y_pred to calculate MSE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-3a5b2b317201>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mret_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0moutput_1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m  \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0moutput_2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'output_1' is not defined"
     ]
    }
   ],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    ret_val = tf.add(tf.reduce_mean( tf.square( output_1 - Y ) ) ,  tf.reduce_mean( tf.square( output_2 - Y ))) \n",
    "    \n",
    "a = tape.gradient(ret_val,output_1)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    mse_val = tf.reshape( tf.reduce_mean( tf.square( output - y_batch ) )  , [ 1 , 1 ] )   \n",
    "\n",
    "a = tape.gradient(mse_val,output)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test of the tape_gradient function. In the cast below, we are going to create a simple function with autodifferentiation which returns the function value, and the derivative of the final results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_X_two' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-b2266ac3d225>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_X_two\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'test_X_two' is not defined"
     ]
    }
   ],
   "source": [
    "type(test_X_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_X_one' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-9eba810b7b84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0moutput_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtest_X_one\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mweights_one\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mbias_1\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0moutput_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtest_X_two\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mweights_two\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mbias_2\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0moutput_1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_Y\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_X_one' is not defined"
     ]
    }
   ],
   "source": [
    "#tf.add(tf.reduce_mean( tf.square( y_pred_1 - Y ) ) ,  tf.reduce_mean( tf.square( y_pred_2 - Y ))\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    output_1 = h( test_X_one , weights_one , bias_1 )\n",
    "    output_2 = h( test_X_two , weights_two , bias_2 )\n",
    "    res = tf.reduce_mean( tf.square( output_1,test_Y ) )\n",
    "    res = tf.add(tf.reduce_mean( tf.square( output_1,test_Y ) ) ,  tf.reduce_mean( tf.square( output_2 - test_Y )))\n",
    "    \n",
    "w_one_grads  = tape.gradient(output_1 , weights_one)  \n",
    "print(w_one_grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_features = data[ [ \"Identification.Year\",\"Engine Information.Engine Statistics.Horsepower\"] ].values / 100 \n",
    "#categorical_research_features = data[ [ 'Research' ] ].values \n",
    "step1_vars = np.concatenate( [ continuous_features ] , axis=1 )\n",
    "step2_vars = np.concatenate( [ data[ [\"Engine Information.Engine Statistics.Torque\"] ] ], axis=1 )\n",
    "dep_var = data[ [ 'Fuel Information.City mpg' ] ].values\n",
    "test_X_one = tf.constant( step1_vars , dtype=tf.float32 ) \n",
    "test_X_two = tf.constant( step2_vars , dtype=tf.float32 ) \n",
    "test_Y = tf.constant( dep_var , dtype=tf.float32 )\n",
    "dataset = tf.data.Dataset.from_tensor_slices(( test_X_one , test_Y )) \n",
    "dataset = dataset.shuffle( 500 ).repeat( num_epochs ).batch( batch_size )\n",
    "iterator = dataset.__iter__()\n",
    "num_features_one = test_X_one.shape[1]\n",
    "num_features_two = test_X_two.shape[1]\n",
    "weights_one = tf.random.normal( ( num_features_one , 1 ) )\n",
    "weights_two = tf.random.normal( ( num_features_two , 1 ) )\n",
    "lambda_var = tf.random.normal( ( 1 , 1 ) )\n",
    "bias_1 = tf.random.normal( ( 1 , 1 ) )\n",
    "bias_2 = tf.random.normal( ( 1 , 1 ) )\n",
    "epochs_plot = list()\n",
    "loss_plot = list()\n",
    "output_1 = tf.Variable(h( test_X_one , weights_one , bias_1 ),tf.float32)\n",
    "output_2 = tf.Variable(h( test_X_one , weights_one , bias_2 ),tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-0.00669864]\n",
      " [-0.00704053]\n",
      " [-0.00704053]\n",
      " ...\n",
      " [-0.00464521]\n",
      " [-0.00626577]\n",
      " [-0.00626577]], shape=(5076, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    \n",
    "    output_1 =tf.Variable( h( test_X_one , weights_one , bias_1 ), dtype=tf.float32 )\n",
    "    output_2 = tf.Variable( h( test_X_two , weights_two , bias_2 ), dtype=tf.float32 )\n",
    "#    res = tf.reduce_mean( tf.square( output_1,test_Y ) )\n",
    "    res = tf.reshape( tf.add(tf.reduce_mean( tf.square( output_1,test_Y ) ) ,  tf.reduce_mean( tf.square( output_2 - test_Y ))) , [ 1 , 1 ] )   \n",
    "    \n",
    "print(tape.gradient(res , output_1)  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([156. 900. 132.], shape=(3,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x_vals = tf.Variable([1.0,2.0,3.0],name = \"x_vals\")\n",
    "x_vals_two = tf.Variable([2.0,3.0,4.0],name = \"x_vals_two\")\n",
    "with tf.GradientTape() as tape:\n",
    "    y = x_vals[0]**3+x_vals[2]*x_vals[1]**2 + 2*x_vals[2]+100\n",
    "    z_one = x_vals[1]*(4*(y) + x_vals[0]**2)    \n",
    "    y_two = x_vals_two[0]**3+x_vals_two[2]*x_vals_two[1]**2 + 2*x_vals_two[2]+100\n",
    "    z_two = x_vals_two[1]*(4*(y_two) + x_vals_two[0]**2)\n",
    "    z = z_one + z_two\n",
    "    \n",
    "a, b = tape.gradient(z , [x_vals,x_vals_two]) \n",
    "print(b)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1938.5131"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean( np.square( output_1 - test_Y ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=1938.5135>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_mean( tf.square( output_1 - test_Y ) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "w1 = tf.Variable(tf.random.normal( ( num_features_one , 1 ) ),name = \"w1\")\n",
    "w2 = tf.Variable(tf.random.normal( ( num_features_two, 1 ) ),name = \"w2\")\n",
    "bias_1 = tf.Variable(tf.random.normal( ( 1 , 1 ) ),name = \"bias_1\")\n",
    "bias_2 = tf.Variable(tf.random.normal( ( 1 , 1 ) ),name = \"bias_2\")\n",
    "output_1 =tf.Variable( h( test_X_one , w1 , bias_1 ), dtype=tf.float32 )\n",
    "output_2 = tf.Variable( h( test_X_two , w2 , bias_2 ), dtype=tf.float32 )\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "#    mse_val = tf.reshape( tf.reduce_mean( tf.square( output_1 - test_Y ) )  , [ 1 , 1 ] )\n",
    "#    mse_val = np.sum(output_1 - test_Y)**2 \n",
    "#    mse_val = w1[0]**2 + w1[1]*2+w1[0]*w1[1] + 10 + w2[0]**3\n",
    "    mse_val =  tf.Variable(np.mean( np.square( output_1 - test_Y ) ),dtype = tf.float32)\n",
    "results = tape.gradient(mse_val,w1)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "output = h( test_X_one , weights_one , bias_1 ) \n",
    "\n",
    "output_1 =tf.Variable( h( test_X_one , weights_one , bias_1 ), dtype=tf.float32 )\n",
    "output_2 = tf.Variable( h( test_X_two , weights_two , bias_2 ), dtype=tf.float32 )\n",
    "#loss = epoch_loss.append( mean_squared_error( test_Y , output_1 ).numpy() )\n",
    "#output = tf.Variable( output_1, dtype=tf.float32 ) \n",
    "y_batch = tf.Variable( test_Y, dtype=tf.float32 ) \n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(weights_one)\n",
    "    mse_val = tf.reshape( tf.reduce_mean( tf.square( output_1 - test_Y ) )  , [ 1 , 1 ] )    \n",
    "    \n",
    "dJ_dH = tape.gradient(mse_val,weights_one)\n",
    "print(dJ_dH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(5076, 1) dtype=float32, numpy=\n",
       "array([[17.685701],\n",
       "       [18.122816],\n",
       "       [18.122816],\n",
       "       ...,\n",
       "       [15.076742],\n",
       "       [17.14866 ],\n",
       "       [17.14866 ]], dtype=float32)>"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'Variable:0' shape=(2, 1) dtype=float32, numpy=\n",
       " array([[ 1.0401548],\n",
       "        [-0.8742276]], dtype=float32)>,\n",
       " <tf.Variable 'Variable:0' shape=(5076, 1) dtype=float32, numpy=\n",
       " array([[17.685701],\n",
       "        [18.122816],\n",
       "        [18.122816],\n",
       "        ...,\n",
       "        [15.076742],\n",
       "        [17.14866 ],\n",
       "        [17.14866 ]], dtype=float32)>)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tape.watched_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as tape:    \n",
    "    output_1 = tf.Variable(h( test_X_one , weights_one , bias_1 ),dtype = tf.float32)\n",
    "    output_2 = tf.Variable( h( test_X_two , weights_two , bias_2 ) ,dtype = tf.float32)\n",
    "    \n",
    "    #    final = tf.tensordot( test_X_one , weights_one , axes=1 ) + bias_1\n",
    "#    output_2 = h( test_X_two , weights_two , bias_2 )\n",
    "   # res = tf.reduce_mean( tf.square( output_1,test_Y ) )\n",
    "#    res = tf.add(tf.reduce_mean( tf.square( output_1,test_Y ) ) ,  tf.reduce_mean( tf.square( output_2 - test_Y )))\n",
    "    \n",
    "w_one_grads  = tape.gradient(output_1 , weights_one)  \n",
    "print(w_one_grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    ret_val = reg_lagrange( Y=y_batch , y_pred_1 = output_1, y_pred_2 = output_2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_plot = list()\n",
    "loss_plot = list()\n",
    "\n",
    "\n",
    "# Define lagrangian here by taking the sum of the two functions.\n",
    "def reg_lagrange( Y , y_pred_1, y_pred_2 ):\n",
    "    return tf.add(tf.reduce_mean( tf.square( y_pred_1 - Y ) ) ,  tf.reduce_mean( tf.square( y_pred_2 - Y ))) \n",
    "\n",
    "\n",
    "for i in range( num_epochs ) :    \n",
    "    epoch_loss = list()\n",
    "    for b in range( int(num_samples/batch_size) ):\n",
    "        x_batch , y_batch = iterator.get_next()\n",
    "#output = h( x_batch , weights , bias ) \n",
    "        output_1 = h( test_X_one , weights_one , bias_1 )\n",
    "        output_2 = h( test_X_one , weights_one , bias_2 )\n",
    "        loss = epoch_loss.append( reg_lagrange( Y=y_batch , y_pred_1 = output_1, y_pred_2 = output_2 ).numpy() )\n",
    "    \n",
    "        output = tf.Variable( output, dtype=tf.float32 ) \n",
    "        y_batch = tf.Variable( y_batch, dtype=tf.float32 ) \n",
    "        with tf.GradientTape() as tape:\n",
    "            mse_val = tf.reshape( tf.reduce_mean( tf.square( output - y_batch ) )  , [ 1 , 1 ] )   \n",
    "    output_1\n",
    "    \n",
    "        with tf.GradientTape() as tape:\n",
    "            ret_val = reg_lagrange( Y=y_batch , y_pred_1 = output_1, y_pred_2 = output_2 )\n",
    "    \n",
    "        dJ_dH = tape.gradient(mse_val,output)\n",
    "        #dJ_dH = mean_squared_error_deriv( y_batch , output)\n",
    "        dH_dW = x_batch\n",
    "        dJ_dW = tf.reduce_mean( dJ_dH * dH_dW )\n",
    "        dJ_dB = tf.reduce_mean( dJ_dH )\n",
    "    \n",
    "        weights -= ( learning_rate * dJ_dW )\n",
    "        bias -= ( learning_rate * dJ_dB ) \n",
    "    loss = np.array( epoch_loss ).mean()\n",
    "    epochs_plot.append( i + 1 )\n",
    "    loss_plot.append( loss ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_features = data[ [ \"Identification.Year\",\"Engine Information.Engine Statistics.Horsepower\"] ].values / 100 \n",
    "#categorical_research_features = data[ [ 'Research' ] ].values \n",
    "step1_vars = np.concatenate( [ continuous_features ] , axis=1 )\n",
    "step2_vars = np.concatenate( [ data[ [\"Engine Information.Engine Statistics.Torque\"] ] ], axis=1 )\n",
    "dep_var = data[ [ 'Fuel Information.City mpg' ] ].values\n",
    "test_X_one = tf.constant( step1_vars , dtype=tf.float32 ) \n",
    "\n",
    "num_epochs = 10\n",
    "num_samples = test_X_one.shape[0]\n",
    "batch_size = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "\n",
    "test_X_two = tf.constant( step2_vars , dtype=tf.float32 ) \n",
    "test_Y = tf.constant( dep_var , dtype=tf.float32 )\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(( test_X_one , test_Y )) \n",
    "dataset = dataset.shuffle( 500 ).repeat( num_epochs ).batch( batch_size )\n",
    "iterator = dataset.__iter__()\n",
    "num_features = test_X_one.shape[1]\n",
    "weights = tf.random.normal( ( num_features , 1 ) )\n",
    "bias = 0\n",
    "\n",
    "epochs_plot = list()\n",
    "loss_plot = list()\n",
    "\n",
    "for i in range( num_epochs ) :    \n",
    "    epoch_loss = list()\n",
    "    for b in range( int(num_samples/batch_size) ):\n",
    "        x_batch , y_batch = iterator.get_next()\n",
    "   \n",
    "        output = h( x_batch , weights , bias ) \n",
    "        loss = epoch_loss.append( mean_squared_error( y_batch , output ).numpy() )\n",
    "    \n",
    "        output = tf.Variable( output, dtype=tf.float32 ) \n",
    "        y_batch = tf.Variable( y_batch, dtype=tf.float32 ) \n",
    "        with tf.GradientTape() as tape:\n",
    "            mse_val = tf.reshape( tf.reduce_mean( tf.square( output - y_batch ) )  , [ 1 , 1 ] )   \n",
    "    \n",
    "    \n",
    "        dJ_dH = tape.gradient(mse_val,output)\n",
    "        #dJ_dH = mean_squared_error_deriv( y_batch , output)\n",
    "        dH_dW = x_batch\n",
    "        dJ_dW = tf.reduce_mean( dJ_dH * dH_dW )\n",
    "        dJ_dB = tf.reduce_mean( dJ_dH )\n",
    "    \n",
    "        weights -= ( learning_rate * dJ_dW )\n",
    "        bias -= ( learning_rate * dJ_dB ) \n",
    "    loss = np.array( epoch_loss ).mean()\n",
    "    epochs_plot.append( i + 1 )\n",
    "    loss_plot.append( loss ) \n",
    "    \n",
    "#    print( 'Loss is {}'.format( loss ) ) \n",
    "\n",
    "#Get predictions and then use them to create residuals.\n",
    "preds = bias +  weights[0]*step1_vars[:,0]+  weights[1]*step1_vars[:,1]     \n",
    "res_vals = preds-dep_var[:,0]        \n",
    "hold = pd.DataFrame(step2_vars)\n",
    "hold[\"res\"] = np.array(res_vals).tolist()\n",
    "test_X_two = tf.constant( np.concatenate( [ hold ], axis=1 ) , dtype=tf.float32 ) \n",
    " \n",
    "dataset_two = tf.data.Dataset.from_tensor_slices(( test_X_two , test_Y )) \n",
    "dataset_two = dataset_two.shuffle( 500 ).repeat( num_epochs ).batch( batch_size )\n",
    "iterator = dataset_two.__iter__()\n",
    "num_features = test_X_two.shape[1]\n",
    "weights = tf.random.normal( ( num_features , 1 ) )\n",
    "bias = 0\n",
    "        \n",
    "for i in range( num_epochs ) :    \n",
    "    epoch_loss = list()\n",
    "    for b in range( int(num_samples/batch_size) ):\n",
    "        x_batch , y_batch = iterator.get_next()\n",
    "   \n",
    "        output = h( x_batch , weights , bias ) \n",
    "        loss = epoch_loss.append( mean_squared_error( y_batch , output ).numpy() )\n",
    "    \n",
    "        output = tf.Variable( output, dtype=tf.float32 ) \n",
    "        y_batch = tf.Variable( y_batch, dtype=tf.float32 ) \n",
    "        with tf.GradientTape() as tape:\n",
    "            mse_val = tf.reshape( tf.reduce_mean( tf.square( output - y_batch ) )  , [ 1 , 1 ] )   \n",
    "    \n",
    "    \n",
    "        dJ_dH = tape.gradient(mse_val,output)\n",
    "        #dJ_dH = mean_squared_error_deriv( y_batch , output)\n",
    "        dH_dW = x_batch\n",
    "        dJ_dW = tf.reduce_mean( dJ_dH * dH_dW )\n",
    "        dJ_dB = tf.reduce_mean( dJ_dH )\n",
    "    \n",
    "        weights -= ( learning_rate * dJ_dW )\n",
    "        bias -= ( learning_rate * dJ_dB )      \n",
    "        \n",
    "        \n",
    "        \n",
    "    loss = np.array( epoch_loss ).mean()\n",
    "    epochs_plot.append( i + 1 )\n",
    "    loss_plot.append( loss ) \n",
    "    print( 'Loss is {}'.format( loss ) ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The version below solves the problem as a lagrange optimization problem:\n",
    "\n",
    "\n",
    "$$ min(\\hat{y_2}  - y)^2 + \\lambda(\\hat{y_1}  - \\beta X)^2$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def reg_lagrange_problem( Y , y_one_pred, y_two_pred ):\n",
    "    return tf.reduce_mean( tf.square( y_pred - Y ) ) + "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = bias +  weights[0]*step1_vars[:,0]+  weights[1]*step1_vars[:,1] \n",
    "res_vals = preds-dep_var[:,0]\n",
    "res_vals = np.array(res_vals)\n",
    "#res_vals.resize([5076,0])\n",
    "res_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(res_vals,step2_vars[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step2_vars[:,0]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.concatenate( [ data[ [\"Engine Information.Engine Statistics.Torque\"] ] ], axis=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hold = pd.DataFrame(step2_vars)\n",
    "hold[\"res\"] = res_vals.tolist()\n",
    "np.concatenate( [ hold ], axis=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.concatenate((step2_vars, res_vals), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(res_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(step2_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.hstack(res_vals,step2_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_reg(indeps,deps,num_epochs = 10,batch_size = 50, learning_rate = .001):\n",
    "    num_samples = indeps.shape[0]\n",
    "    indeps = tf.constant( indeps , dtype=tf.float32 ) \n",
    "    deps = tf.constant( deps , dtype=tf.float32 ) \n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices(( indeps , deps )) \n",
    "    dataset = dataset.shuffle( 500 ).repeat( num_epochs ).batch( batch_size )\n",
    "    iterator = dataset.__iter__()\n",
    "    num_features = indeps.shape[1]\n",
    "    weights = tf.random.normal( ( num_features , 1 ) )\n",
    "    bias = 0\n",
    "    epochs_plot = list()\n",
    "    loss_plot = list()\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_vals = bias +  weights[0]*step1_vars[:,0]+  weights[1]*step1_vars[:,1] - dep_var\n",
    "pred_vals - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
